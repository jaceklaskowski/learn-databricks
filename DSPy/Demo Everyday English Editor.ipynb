{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33aa5c5-5f70-4646-b7c4-fc25cb09d92d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Demo: Everyday English Editor\n",
    "\n",
    "Influenced by [10-minute demo: Evaluate a GenAI app](https://docs.databricks.com/aws/en/mlflow3/genai/getting-started/eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e95d15-9a8c-471d-b45e-a53df2d70b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb4ce4c-6101-43c6-8051-781b9aca4f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU dspy>=3.0.4 mlflow>=3.8.1\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e3e4139-cdd0-4a63-90b7-9c1ba6cc9631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd21f0a-e53b-44fa-9e3d-a049342baf96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "print(f\"DSPy version: {dspy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daed1f6d-5492-42bf-bd07-300876dd984a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enable GenAI Observability with MLflow Tracing\n",
    "\n",
    "[MLflow Tracing - GenAI observability](https://docs.databricks.com/aws/en/mlflow3/genai/tracing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669aec53-fff5-44ce-9e1b-96f4bca071a3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enable MLflow DSPy Autologging"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.dspy.autolog(\n",
    "    log_traces=True,\n",
    "    log_traces_from_compile=True,\n",
    "    log_traces_from_eval=True,\n",
    "    log_compiles=True,\n",
    "    log_evals=True,\n",
    "    disable=False,\n",
    "    silent=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7204acf-2905-4425-b0f8-afcb2aa31b7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Up LM"
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "model_serving_endpoint_name = \"databricks-claude-sonnet-4-5\"\n",
    "\n",
    "lm = dspy.LM(\n",
    "    model=f\"databricks/{model_serving_endpoint_name}\",\n",
    ")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0738cd73-d741-4d69-a5bd-4f8dfe7ccc23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DSPy Program\n",
    "\n",
    "Let's create a DSPy program (_task_) to rephrase an English sentence to sound like an English native speaker and making it sound more natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dad4f716-e9bb-4bbb-916b-58524365ea6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class EverydayEnglishEditorSignature(dspy.Signature):\n",
    "    \"\"\"Everyday English Rewriter that makes sentences sound more natural.\n",
    "    The result is grammatically correct, but, more importantly, culturally natural.\n",
    "    Rewrites formal or awkward sentences to make them sound like something a native speaker would actually say in a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    sentence: str = dspy.InputField()\n",
    "    natural_english_sentence: str = dspy.OutputField()\n",
    "\n",
    "\n",
    "everyday_english_editor = dspy.ChainOfThought(\n",
    "    signature=EverydayEnglishEditorSignature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f60fb86-9492-4e09-898f-b9e61c7990ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"In this tutorial notebook, we use databricks/databricks-claude-sonnet-4-5 (a Databricks-hosted Claude Sonnet 4.5 from Anthropic)\"\n",
    "\n",
    "# the result is called a prediction\n",
    "prediction = everyday_english_editor(sentence=sentence)\n",
    "\n",
    "print(f\"natural_english_sentence:\\n{prediction.natural_english_sentence}\")\n",
    "print(f\"Reasoning:\\n{prediction.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96abd37d-3ae8-4f1e-86a2-e1136b760345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GenAI Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efae251-d2cd-4347-b697-c429670e5280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9507effa-83ee-49dd-83a8-6c5fffaf3f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_data = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"sentence\": \"Yesterday, ____ (person) brought a ____ (item) and used it to ____ (verb) a ____ (object)\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"sentence\": \"I wanted to ____ (verb) but ____ (person) told me to ____ (verb) instead\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"sentence\": \"The ____ (adjective) ____ (animal) likes to ____ (verb) in the ____ (place)\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"sentence\": \"My favorite ____ (food) is made with ____ (ingredient) and ____ (ingredient)\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"sentence\": \"When I grow up, I want to be a ____ (job) who can ____ (verb) all day\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"sentence\": \"When two ____ (animals) love each other, they ____ (verb) under the ____ (place)\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"sentence\": \"The monster wanted to ____ (verb) all the ____ (plural noun) with its ____ (body part)\"\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2aff535-3d49-4281-ad14-0eca5e295296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Evaluation Criteria\n",
    "\n",
    "In this step, you set up [scorers](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/concepts/scorers) to evaluate the quality of the completions based on the following:\n",
    "\n",
    "* Language consistency: Same language as input.\n",
    "* Creativity: Funny or creative responses.\n",
    "* Child safety: Age-appropriate content.\n",
    "* Template structure: Fills blanks without changing format.\n",
    "* Content safety: No harmful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329793c4-503a-438e-a947-794111892f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines, Safety\n",
    "import mlflow.genai\n",
    "\n",
    "# Define evaluation scorers\n",
    "scorers = [\n",
    "    Guidelines(\n",
    "        guidelines=\"Response must be in the same language as the input\",\n",
    "        name=\"same_language\",\n",
    "    ),\n",
    "    Guidelines(\n",
    "        guidelines=\"Response must be funny or creative\",\n",
    "        name=\"funny\"\n",
    "    ),\n",
    "    Guidelines(\n",
    "        guidelines=\"Response must be appropiate for children\",\n",
    "        name=\"child_safe\"\n",
    "    ),\n",
    "    Guidelines(\n",
    "        guidelines=\"Response must follow the input template structure from the request.\",\n",
    "        name=\"template_match\",\n",
    "    ),\n",
    "    Safety(),  # Built-in safety scorer\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32fa1283-1e14-4149-8e2f-de42b5c884ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659bbf74-cca1-4dd4-99b2-8054663ad0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating with basic prompt...\")\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    predict_fn=everyday_english_editor,\n",
    "    scorers=scorers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf955954-8807-43f1-83cc-70301bd4cb7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View Evaluation Results in Experiments UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f094eb-5b4b-4616-bbcb-c987ef7b7ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DSPy Optimizers\n",
    "\n",
    "[DSPy Optimizers (formerly Teleprompters)](https://dspy.ai/learn/optimization/optimizers/):\n",
    "\n",
    "A **DSPy optimizer** is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics of your choice (like accuracy).\n",
    "\n",
    "DSPy optimizers are subclasses of `dspy.Teleprompter`\n",
    "\n",
    "| DSPy Optimizer | Description |\n",
    "|-|-|\n",
    "| `dspy.BetterTogether` | Experimental optimizer.<br>Supports `BootstrapFinetune` and `BootstrapFewShotWithRandomSearch` optimizers only. |\n",
    "| `dspy.BootstrapFewShot` | [Automatic Few-Shot Learning](https://dspy.ai/learn/optimization/optimizers/#automatic-few-shot-learning)<br>Composes a set of demos/examples to go into a predictor's prompt.<br>These demos come from a combination of labeled examples in the training set, and bootstrapped demos. |\n",
    "| `dspy.BootstrapFewShotWithRandomSearch` | [Automatic Few-Shot Learning](https://dspy.ai/learn/optimization/optimizers/#automatic-few-shot-learning) |\n",
    "| `dspy.BootstrapFinetune` | [Automatic Finetuning](https://dspy.ai/learn/optimization/optimizers/#automatic-finetuning)<br>Fine-tunes LM weights. |\n",
    "| `dspy.BootstrapRS` | synthesizing good few-shot examples |\n",
    "| `dspy.COPRO` | [Automatic Instruction Optimization](https://dspy.ai/learn/optimization/optimizers/#automatic-instruction-optimization) |\n",
    "| `dspy.Ensemble` | [Program Transformations](https://dspy.ai/learn/optimization/optimizers/#program-transformations) |\n",
    "| `dspy.GEPA` | [Automatic Instruction Optimization](https://dspy.ai/learn/optimization/optimizers/#automatic-instruction-optimization)<br>Evolutionary experimental optimizer.<br>Uses reflection to evolve text components of complex systems.<br>Proposed in the paper [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457) |\n",
    "| `dspy.KNNFewShot` | [Automatic Few-Shot Learning](https://dspy.ai/learn/optimization/optimizers/#automatic-few-shot-learning)<br>Uses an in-memory KNN retriever to find the `k` nearest neighbors in a trainset at test time.<br>For each input example in a `forward()` call, it identifies the `k` most similar examples from the trainset and attaches them as demonstrations to the student module. |\n",
    "| `dspy.InferRules` | &nbsp; |\n",
    "| `dspy.LabeledFewShot` | [Automatic Few-Shot Learning](https://dspy.ai/learn/optimization/optimizers/#automatic-few-shot-learning)<br>Constructs few-shot examples (demos) from provided labeled input and output data points. |\n",
    "| `dspy.MIPROv2` | [Automatic Instruction Optimization](https://dspy.ai/learn/optimization/optimizers/#automatic-instruction-optimization)<br>Multiprompt Instruction PRoposal Optimizer Version 2<br>Proposing and intelligently exploring better natural-language instructions for every prompt.<br>Leading prompt optimizer. |\n",
    "| `dspy.SIMBA` | [Automatic Instruction Optimization](https://dspy.ai/learn/optimization/optimizers/#automatic-instruction-optimization)<br>SIMBA (Stochastic Introspective Mini-Batch Ascent) optimizer for DSPy.<br>Uses a LLM to analyze its own performance and generate improvement rules. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27889934-aeb9-423a-8800-b16a6f9c4cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import Teleprompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e7bb2f-2dc0-4482-8df8-9b5669455c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dspy_program = everyday_english_editor\n",
    "for predictor in dspy_program.predictors():\n",
    "    print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c17a6abb-1063-43da-ae20-38bbf3f9c6a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prompt Optimization with MIPROv2\n",
    "\n",
    "Per [this DSPy doc](https://dspy.ai/learn/optimization/optimizers/#which-optimizer-should-i-use):\n",
    "\n",
    "> If you prefer to do instruction optimization only (i.e., you want to keep your prompt 0-shot), use MIPROv2 configured for 0-shot optimization.\n",
    "\n",
    "[Reflective Prompt Evolution with GEPA](https://dspy.ai/tutorials/gepa_ai_program/):\n",
    "\n",
    "> GEPA proposes new prompts, building a tree of evolved prompt candidates, accumulating improvements as the optimization progresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ddaa7e7-fff2-48b8-ae91-83a959d1ee04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## MIPROv2\n",
    "\n",
    "**MIPROv2** (<b>M</b>ultiprompt <b>I</b>nstruction <b>PR</b>oposal <b>O</b>ptimizer Version 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f2c706-0402-413b-af7b-c86aaa102d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's create a list of `dspy.Example`s, which is the datatype that carries training (or test) datapoints in DSPy.\n",
    "\n",
    "When you build a `dspy.Example`, you should generally specify `.with_inputs(\"field1\", \"field2\", ...)` to indicate which fields are inputs.\n",
    "\n",
    "The other fields are treated as labels or metadata.\n",
    "\n",
    "For prompt optimizers in particular, it's often better to pass _more_ validation than training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da72cc92-cc74-45df-ba1b-2eb2a3f3e01f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"sentence\": \"I would like to inform you that I have completed the task which was assigned to me yesterday.\",\n",
    "        \"answer\": \"Just letting you know I finished the task you gave me yesterday.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"It is my understanding that this solution will not be sufficient for the requirements of the project.\",\n",
    "        \"answer\": \"I don't think this solution will meet the project's requirements.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "data = [dspy.Example(**d).with_inputs(\"sentence\") for d in data]\n",
    "trainset = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389624db-a48a-4b8a-884c-0f484b2a7dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In DSPy, there is no single \"hardcoded\" similarity metric for the MIPROv2 optimizer.\n",
    "\n",
    "Instead, you define a custom metric function that returns a score (usually between 0 and 1) representing how \"good\" a prediction is compared to a reference.\n",
    "\n",
    "For calculating similarity between two sentences specifically, you should choose a metric based on how strict or semantic you want the comparison to be:\n",
    "\n",
    "1. Semantic Similarity (Recommended)\n",
    "    1. Using [sentence_transformers.SentenceTransformers](https://sbert.net/) You can manually compute cosine similarity between embeddings\n",
    "    1. LLM-as-a-Judge (High Accuracy) For complex nuance, you can use a smaller LLM as a judge within your metric function to \"score\" the similarity.\n",
    "1. Lexical/String Similarity (Fast & Simple)\n",
    "    1. F1 Score: Good for balancing precision and recall of words.\n",
    "    1. Exact Match: Use dspy.evaluate.answer_exact_match for strict tasks (like code or math).\n",
    "    1. [difflib.SequenceMatcher](https://docs.python.org/3/library/difflib.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d700eae-f17d-4c27-babc-af7aa02a6165",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LLM-as-a-Judge (High Accuracy)"
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class AssessSimilarity(dspy.Signature):\n",
    "    \"\"\"Rate the semantic similarity between two sentences on a scale of 0 to 1.\"\"\"\n",
    "    gold_answer = dspy.InputField()\n",
    "    predicted_answer = dspy.InputField()\n",
    "    rating = dspy.OutputField(desc=\"A float between 0 and 1\")\n",
    "\n",
    "judge = dspy.Predict(AssessSimilarity)\n",
    "\n",
    "def llm_metric(example, pred, trace=None):\n",
    "    return float(judge(gold_answer=example.answer, predicted_answer=pred.natural_english_sentence).rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5664167-acf7-4799-b87c-49959adcea31",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optimize Prompt Parameters"
    }
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "optimizer = MIPROv2(\n",
    "    metric=llm_metric,\n",
    ")\n",
    "\n",
    "# Optimizing instructions only with MIPROv2 (0-Shot)\n",
    "# zero-shot is when both max_bootstrapped_demos and max_labeled_demos are 0\n",
    "optimized_everyday_english_editor = optimizer.compile(\n",
    "    everyday_english_editor,\n",
    "    trainset=trainset,\n",
    "    # default: 4\n",
    "    max_bootstrapped_demos=0,\n",
    "    # default: 4\n",
    "    max_labeled_demos=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9403a91-9331-4ecc-9e18-57c37d1b7364",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Optimized DSPy Program"
    }
   },
   "outputs": [],
   "source": [
    "# optimized_everyday_english_editor.save(\n",
    "#     path=\"everyday_english_editor\",\n",
    "#     # Save the whole module to a directory via cloudpickle,\n",
    "#     # which contains both the state and architecture of the model.\n",
    "#     save_program=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e81600f-84a0-4766-b520-cf39f305e00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "everyday_english_editor(sentence=\"I want to go to the beach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e5c242-d97e-4fd9-8e11-5f35006ddf63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimized_everyday_english_editor(sentence=\"I want to go to the beach.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7502507528156634,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo Everyday English Editor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
