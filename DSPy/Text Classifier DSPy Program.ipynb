{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d553fd9-0c4f-4422-ab83-5b9f043fc848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Text Classifier DSPy Program\n",
    "\n",
    "Based on [Build generative AI apps using DSPy on Databricks](https://docs.databricks.com/aws/en/generative-ai/dspy)\n",
    "\n",
    "This demo notebook shows how to use DSPy on Databricks to build and optimize AI agents (_agentic systems_).\n",
    "\n",
    "DSPy comes with the following features:\n",
    "\n",
    "1. Automate prompt engineering (replacing manual text-templating prompt engineering with structured Python-based \"text transformation graphs\").\n",
    "1. Orchestrate LLM fine-tuning to improve performance.\n",
    "\n",
    "DSPy consists of the following building blocks for agent development and improving agent quality:\n",
    "\n",
    "1. **Modules**: In DSPy, these are components that handle specific text transformations, like answering questions or summarizing. They replace traditional hand-written prompts and can learn from examples, making them more adaptable.\n",
    "1. **Signatures**: A natural language description of a module's input and output behavior. For example, â€œquestion -> answerâ€ specifies that the module should take a question as input and return an answer.\n",
    "1. **Compiler**: This is DSPy's optimization tool. It improves LM pipelines by adjusting modules to meet a performance metric, either by generating better prompts or fine-tuning models.\n",
    "1. **Program (DSPy)**: A set of modules connected into a pipeline to perform complex tasks. DSPy programs are flexible, allowing you to optimize and adapt them using the compiler.\n",
    "\n",
    "Let's create a DSPy program for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84283330-e4f5-4df5-a66d-2ab64f997ccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c447d9fa-7eaf-454a-a9b7-17ae482f2c9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU dspy>=3.0.4 mlflow>=3.7.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243be0df-0d82-4083-a1de-0218297316df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a96f824c-a85b-46f0-935d-0b551c01281e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "print(f\"DSPy version: {dspy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6abd9b-68a1-4577-8eba-7b2dfd3eb2fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Choose LM\n",
    "\n",
    "We'll use one of the Databricks foundational model serving endpoints.\n",
    "\n",
    "Let's list the available serving endpoints (using MLflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c3f67e-f77e-424f-9f2a-89c9483e6781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "\n",
    "deploy_client = get_deploy_client(\"databricks\")\n",
    "endpoints = deploy_client.list_endpoints()\n",
    "\n",
    "endpoint_names = sorted([endpoint['name'] for endpoint in endpoints])\n",
    "\n",
    "# Wrap each name in a tuple to match the schema to build a DataFrame from\n",
    "endpoint_names_df = spark.createDataFrame(\n",
    "    [(name,) for name in endpoint_names],\n",
    "    schema=StructType([StructField(\"Endpoint Name\", StringType())])\n",
    ")\n",
    "\n",
    "display(endpoint_names_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f027277-19cb-453d-bb8e-a3a4f0c15099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "model_serving_endpoint_name = \"databricks-gpt-5-2\"\n",
    "\n",
    "# Leverage default authentication inside the Databricks context (notebooks, workflows, etc.)\n",
    "lm = dspy.LM(\n",
    "    model=f\"databricks/{model_serving_endpoint_name}\",\n",
    ")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "febc38be-f6e3-47aa-acc0-470d0beffa90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The following downloads the [Reuters 21578](https://huggingface.co/datasets/yangwang825/reuters-21578) dataset from Huggingface and defines a utility to ensure that your train and test splits have the same labels.\n",
    "\n",
    "Do I understand what the code does?! Not really ðŸ¤·â€â™‚ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af64b97e-6009-4dc9-892c-bc5b0ce4c05c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dspy.datasets.dataset import Dataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "def read_data_and_subset_to_categories() -> tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Read the reuters-21578 dataset. Docs can be found in the url below:\n",
    "    https://huggingface.co/datasets/yangwang825/reuters-21578\n",
    "    \"\"\"\n",
    "\n",
    "    # Read train/test split\n",
    "    file_path = \"hf://datasets/yangwang825/reuters-21578/{}.json\"\n",
    "    train = pd.read_json(file_path.format(\"train\"))\n",
    "    test = pd.read_json(file_path.format(\"test\"))\n",
    "\n",
    "    # Clean the labels\n",
    "    label_map = {\n",
    "        0: \"acq\",\n",
    "        1: \"crude\",\n",
    "        2: \"earn\",\n",
    "        3: \"grain\",\n",
    "        4: \"interest\",\n",
    "        5: \"money-fx\",\n",
    "        6: \"ship\",\n",
    "        7: \"trade\",\n",
    "    }\n",
    "\n",
    "    train[\"label\"] = train[\"label\"].map(label_map)\n",
    "    test[\"label\"] = test[\"label\"].map(label_map)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_train_per_label: int = 20,\n",
    "        n_test_per_label: int = 10,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_train_per_label = n_train_per_label\n",
    "        self.n_test_per_label = n_test_per_label\n",
    "\n",
    "        self._create_train_test_split_and_ensure_labels()\n",
    "\n",
    "    def _create_train_test_split_and_ensure_labels(self) -> None:\n",
    "        \"\"\"Perform a train/test split that ensure labels in `dev` are also in `train`.\"\"\"\n",
    "        # Read the data\n",
    "        train_df, test_df = read_data_and_subset_to_categories()\n",
    "\n",
    "        # Sample for each label\n",
    "        train_samples_df = pd.concat([\n",
    "            group.sample(n=self.n_train_per_label) \n",
    "            for _, group in train_df.groupby('label')\n",
    "        ])\n",
    "        test_samples_df = pd.concat([\n",
    "            group.sample(n=self.n_test_per_label) \n",
    "            for _, group in test_df.groupby('label')\n",
    "        ])\n",
    "\n",
    "        # Set DSPy class variables\n",
    "        self._train = train_samples_df.to_dict(orient=\"records\")\n",
    "        self._dev = test_samples_df.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# Limit to a small dataset to showcase the value of bootstrapping\n",
    "dataset = CSVDataset(n_train_per_label=3, n_test_per_label=1)\n",
    "\n",
    "# Create train and test sets containing DSPy\n",
    "# Note that we must specify the expected input value name\n",
    "train_dataset = [example.with_inputs(\"text\") for example in dataset.train]\n",
    "test_dataset = [example.with_inputs(\"text\") for example in dataset.dev]\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "print(f\"Train labels: {set([example.label for example in dataset.train])}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f5faa74-6ae3-433f-93ba-5f4a626609aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DSPy Program\n",
    "\n",
    "Finally, define your text classification task.\n",
    "\n",
    "There are a variety of ways you can provide guidelines to DSPy signature behavior. Currently, DSPy allows users to specify:\n",
    "\n",
    "1. A high-level goal via the class docstring.\n",
    "1. A set of input fields, with optional metadata.\n",
    "1. A set of output fields with optional metadata.\n",
    "\n",
    "DSPy leverages this information to inform optimization.\n",
    "\n",
    "In the following example, notice that there is not information in the `TextClassificationSignature` class about the dataset of interest. This means you are effectively leveraging a base LLM instructed to classify text with no context. From this blank slate, you can use DSPy to learn to classify from training alone.\n",
    "\n",
    "Note that in production, you should provide metadata in the signature to reduce training time. This secenario is just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f59d52-b48e-480b-842a-3f015cb30c6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class TextClassificationSignature(dspy.Signature):\n",
    "    text = dspy.InputField()\n",
    "    label = dspy.OutputField(desc=\"Label of predicted_class\")\n",
    "\n",
    "\n",
    "# FIXME This superfluous module is likely not needed\n",
    "class TextClassifier(dspy.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.generate_classification = dspy.Predict(TextClassificationSignature)\n",
    "\n",
    "    def forward(self, text: str):\n",
    "      return self.generate_classification(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92759067-9825-4977-9018-bcae31e44ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Test Classification\n",
    "\n",
    "The DSPy program incorrectly classifies the text because it has not been given the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d437825b-8a92-4150-af14-7d7ff1448601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_classifier = TextClassifier()\n",
    "\n",
    "message = \"I am interested in space\"\n",
    "print(text_classifier(text=message))\n",
    "\n",
    "message = \"I enjoy ice skating\"\n",
    "print(text_classifier(text=message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7352c22-ac9f-4cde-aa08-ef951c5621b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DSPy Training\n",
    "\n",
    "For training, you can use `BootstrapFewShotWithRandomSearch`, an optimizer that takes bootstrap samples from your training set and leverages a random search strategy to optimize your predictive accuracy.\n",
    "\n",
    "Note that the following example uses the simple metric definition of exact match, as defined in `validate_classification`, but `dspy.Metrics` can contain complex and LM-based logic to properly evaluate our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdeb1422-f34c-4323-a909-d6ad4e9ab8b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "def validate_classification(example, prediction, trace=None) -> bool:\n",
    "    return example.label == prediction.label\n",
    "\n",
    "optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric=validate_classification,\n",
    "    num_candidate_programs=5,\n",
    "    max_bootstrapped_demos=2,\n",
    "    num_threads=1,\n",
    ")\n",
    "\n",
    "compiled_pe = optimizer.compile(TextClassifier(), trainset=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84e88253-16f8-493f-acf6-24021093295f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Understand DSPy Training\n",
    "\n",
    "Understanding the path that DSPy training returns can be challenging, however there are a few APIs exposed.\n",
    "\n",
    "The following shows a way to print a formatted and color-coded message history for a number of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7b0ef3-1b0a-46f9-a32b-a19ea1b6f776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71677db7-7921-404c-90d4-5f4b1ca35513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "âš ï¸ There's [MLflow Tracing](https://mlflow.org/docs/latest/genai/tracing/), too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64e01fa9-4720-4197-a119-5ba0b72b7e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Compare Model Accuracy\n",
    "\n",
    "Let's review how well your newly-trained model can predict on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a1197c-daaf-4667-acf9-cae3251fc766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_accuracy(classifier, test_data: pd.DataFrame = test_dataset) -> float:\n",
    "    residuals = []\n",
    "    predictions = []\n",
    "    for example in test_data:\n",
    "        prediction = classifier(text=example[\"text\"])\n",
    "        residuals.append(int(validate_classification(example, prediction)))\n",
    "        predictions.append(prediction)\n",
    "    return residuals, predictions\n",
    "\n",
    "uncompiled_residuals, uncompiled_predictions = check_accuracy(TextClassifier())\n",
    "print(f\"Uncompiled accuracy: {np.mean(uncompiled_residuals)}\")\n",
    "  \n",
    "compiled_residuals, compiled_predictions = check_accuracy(compiled_pe)\n",
    "print(f\"Compiled accuracy: {np.mean(compiled_residuals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d561d03f-1f5c-452d-83fa-772778bf9f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "I've got no idea what these accuracy numbers really mean.\n",
    "\n",
    "Quoting the source notebook (with the results adjusted):\n",
    "\n",
    "> As shown above, your uncompiled accuracy is 0 - your base LLM was not aware of the classification labels.\n",
    ">\n",
    "> However, with training alone, the prompts, demonstrations, and input and output signatures have been updated to prompt your model to reach 100% accuracy (astonishing!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e067e8f3-7d0a-40db-a653-9480b9c6fa75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for uncompiled_residual, uncompiled_prediction in zip(uncompiled_residuals, uncompiled_predictions):\n",
    "    is_correct = \"Correct\" if bool(uncompiled_residual) else \"Incorrect\"\n",
    "    prediction = uncompiled_prediction.label\n",
    "    print(f\"{is_correct} prediction: {' ' * (12 - len(is_correct))}{prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f979b11e-eb31-44ef-89a7-62c63dbdeb05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for compiled_residual, compiled_prediction in zip(compiled_residuals, compiled_predictions):\n",
    "    is_correct = \"Correct\" if bool(compiled_residual) else \"Incorrect\"\n",
    "    prediction = compiled_prediction.label\n",
    "    print(f\"{is_correct} prediction: {' ' * (12 - len(is_correct))}{prediction}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Text Classifier DSPy Program",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
