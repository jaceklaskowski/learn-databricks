{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f9cb61e-e469-425d-960f-6f3f736a4422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Copy of XGBoost MLflow 3 Tutorial\n",
    "\n",
    "Based on [XGBoost MLflow 3 Tutorial](https://docs.databricks.com/aws/en/notebooks/source/mlflow/mlflow-classic-ml-e2e-mlflow-3.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33209774-bd8a-4726-b852-1b47c2e2ff3c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install the latest version of MLflow"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade -Uqqq mlflow>=3.1 xgboost optuna uv\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ca2e11-e6fa-4bef-b9e6-bd32b19c4f0f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1a2091-568c-438d-8050-51851c2d4372",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "0. Configure the Model Registry with Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fe5cf0-999b-45ff-a8ff-f66c1c1bede4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1. Create a synthetic regression dataset"
    }
   },
   "outputs": [],
   "source": [
    "def create_regression_data(\n",
    "    n_samples: int, \n",
    "    n_features: int,\n",
    "    seed: int = 1994,\n",
    "    noise_level: float = 0.3,\n",
    "    nonlinear: bool = True\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Generates synthetic regression data with interesting correlations for MLflow and XGBoost demonstrations.\n",
    "\n",
    "    This function creates a DataFrame of continuous features and computes a target variable with nonlinear\n",
    "    relationships and interactions between features. The data is designed to be complex enough to demonstrate\n",
    "    the capabilities of XGBoost, but not so complex that a reasonable model can't be learned.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): Number of samples (rows) to generate.\n",
    "        n_features (int): Number of feature columns.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 1994.\n",
    "        noise_level (float, optional): Level of Gaussian noise to add to the target. Defaults to 0.3.\n",
    "        nonlinear (bool, optional): Whether to add nonlinear feature transformations. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.Series]:\n",
    "            - pd.DataFrame: DataFrame containing the synthetic features.\n",
    "            - pd.Series: Series containing the target labels.\n",
    "\n",
    "    Example:\n",
    "        >>> df, target = create_regression_data(n_samples=1000, n_features=10)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # Generate random continuous features\n",
    "    X = rng.uniform(-5, 5, size=(n_samples, n_features))\n",
    "    \n",
    "    # Create feature DataFrame with meaningful names\n",
    "    columns = [f\"feature_{i}\" for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    \n",
    "    # Generate base target variable with linear relationship to a subset of features\n",
    "    # Use only the first n_features//2 features to create some irrelevant features\n",
    "    weights = rng.uniform(-2, 2, size=n_features//2)\n",
    "    target = np.dot(X[:, :n_features//2], weights)\n",
    "    \n",
    "    # Add some nonlinear transformations if requested\n",
    "    if nonlinear:\n",
    "        # Add square term for first feature\n",
    "        target += 0.5 * X[:, 0]**2\n",
    "        \n",
    "        # Add interaction between the second and third features\n",
    "        if n_features >= 3:\n",
    "            target += 1.5 * X[:, 1] * X[:, 2]\n",
    "        \n",
    "        # Add sine transformation of fourth feature\n",
    "        if n_features >= 4:\n",
    "            target += 2 * np.sin(X[:, 3])\n",
    "        \n",
    "        # Add exponential of fifth feature, scaled down\n",
    "        if n_features >= 5:\n",
    "            target += 0.1 * np.exp(X[:, 4] / 2)\n",
    "            \n",
    "        # Add threshold effect for sixth feature\n",
    "        if n_features >= 6:\n",
    "            target += 3 * (X[:, 5] > 1.5).astype(float)\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = rng.normal(0, noise_level * target.std(), size=n_samples)\n",
    "    target += noise\n",
    "    \n",
    "    # Add a few more interesting features to the DataFrame\n",
    "    \n",
    "    # Add a correlated feature (but not used in target calculation)\n",
    "    if n_features >= 7:\n",
    "        df['feature_correlated'] = df['feature_0'] * 0.8 + rng.normal(0, 0.2, size=n_samples)\n",
    "    \n",
    "    # Add a cyclical feature\n",
    "    df['feature_cyclical'] = np.sin(np.linspace(0, 4*np.pi, n_samples))\n",
    "    \n",
    "    # Add a feature with outliers\n",
    "    df['feature_with_outliers'] = rng.normal(0, 1, size=n_samples)\n",
    "    # Add outliers to ~1% of samples\n",
    "    outlier_idx = rng.choice(n_samples, size=n_samples//100, replace=False)\n",
    "    df.loc[outlier_idx, 'feature_with_outliers'] = rng.uniform(10, 15, size=len(outlier_idx))\n",
    "    \n",
    "    return df, pd.Series(target, name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c785e22c-207a-4d81-b717-23bd3c46272e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2. Exploratory data analysis (EDA) visualizations"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_distributions(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of histograms for each feature in the dataset.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing synthetic features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the distribution plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            sns.histplot(X[feature], ax=ax, kde=True, color='skyblue')\n",
    "            ax.set_title(f'Distribution of {feature}')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature Distributions', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_heatmap(X: pd.DataFrame, y: pd.Series) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a correlation heatmap of all features and the target variable.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the heatmap.\n",
    "    \"\"\"\n",
    "    # Combine features and target into one DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Draw the heatmap with a color bar\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap=cmap,\n",
    "                center=0, square=True, linewidths=0.5, ax=ax)\n",
    "    \n",
    "    ax.set_title('Feature Correlation Heatmap', fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_feature_target_relationships(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of scatter plots showing the relationship between each feature and the target.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the relationship plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Scatter plot with regression line\n",
    "            sns.regplot(x=X[feature], y=y, ax=ax, \n",
    "                       scatter_kws={'alpha': 0.5, 'color': 'blue'}, \n",
    "                       line_kws={'color': 'red'})\n",
    "            ax.set_title(f'{feature} vs Target')\n",
    "    \n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature vs Target Relationships', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_pairwise_relationships(X: pd.DataFrame, y: pd.Series, features: list[str]) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a pairplot showing relationships between selected features and the target.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        features (List[str]): List of feature names to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the pairplot.\n",
    "    \"\"\"\n",
    "    # Ensure features exist in the DataFrame\n",
    "    valid_features = [f for f in features if f in X.columns]\n",
    "    \n",
    "    if not valid_features:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.text(0.5, 0.5, \"No valid features provided\", ha='center', va='center')\n",
    "        return fig\n",
    "    \n",
    "    # Combine selected features and target\n",
    "    data = X[valid_features].copy()\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Create pairplot\n",
    "    pairgrid = sns.pairplot(data, diag_kind=\"kde\", \n",
    "                          plot_kws={\"alpha\": 0.6, \"s\": 50},\n",
    "                          corner=True)\n",
    "    \n",
    "    pairgrid.fig.suptitle(\"Pairwise Feature Relationships\", y=1.02, fontsize=16)\n",
    "    plt.close(pairgrid.fig)\n",
    "    return pairgrid.fig\n",
    "\n",
    "def plot_boxplots(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of box plots for each feature, with points colored by target value.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the box plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    # Create target bins for coloring\n",
    "    y_binned = pd.qcut(y, 3, labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Box plot for each feature\n",
    "            sns.boxplot(x=y_binned, y=X[feature], ax=ax)\n",
    "            ax.set_title(f'Distribution of {feature} by Target Range')\n",
    "            ax.set_xlabel('Target Range')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature Distributions by Target Range', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_outliers(X: pd.DataFrame, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of box plots to detect outliers in each feature.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the outlier plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Box plot to detect outliers\n",
    "            sns.boxplot(x=X[feature], ax=ax, color='skyblue')\n",
    "            ax.set_title(f'Outlier Detection for {feature}')\n",
    "            ax.set_xlabel(feature)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Outlier Detection for Features', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb88930-85ca-4983-a8fc-3142f284ddd1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3. Standard modeling workflow"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "n_features = 10\n",
    "X, y = create_regression_data(n_samples=n_samples, n_features=n_features, nonlinear=True)\n",
    "\n",
    "# Create EDA plots\n",
    "dist_plot = plot_feature_distributions(X, y)\n",
    "corr_plot = plot_correlation_heatmap(X, y)\n",
    "scatter_plot = plot_feature_target_relationships(X, y)\n",
    "corr_with_target = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "top_features = corr_with_target.head(4).index.tolist()\n",
    "pairwise_plot = plot_pairwise_relationships(X, y, top_features)\n",
    "outlier_plot = plot_outliers(X)\n",
    "\n",
    "# Configure the XGBoost model\n",
    "reg = xgb.XGBRegressor(\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='rmse',\n",
    ")\n",
    "\n",
    "# Create train/test split to properly evaluate the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7722)\n",
    "\n",
    "# Train the model with evaluation\n",
    "reg.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate predictions for residual plot\n",
    "y_pred = reg.predict(X_test)\n",
    "residual_plot = plot_boxplots(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ae5b9c-63f7-420f-99a2-516210d994f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "4. Log the model using MLflow"
    }
   },
   "outputs": [],
   "source": [
    "# Incorporate MLflow evaluation\n",
    "evaluation_data = X_test.copy()\n",
    "evaluation_data[\"label\"] = y_test\n",
    "\n",
    "# Log the model and training metadata results\n",
    "with mlflow.start_run() as run:\n",
    "    # Extract metrics\n",
    "    final_train_rmse = np.array(reg.evals_result()[\"validation_0\"][\"rmse\"])[-1]\n",
    "    final_test_rmse = np.array(reg.evals_result()[\"validation_1\"][\"rmse\"])[-1]\n",
    "    \n",
    "    # Extract parameters for logging\n",
    "    feature_map = {key: value for key, value in reg.get_xgb_params().items() if value is not None}\n",
    "\n",
    "    # Generate a model signature using the infer_signature utility in MLflow\n",
    "    # A signature is required to register the model to Unity Catalog \n",
    "    # so that the model can be used in SQL queries\n",
    "    signature = infer_signature(X, reg.predict(X))\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params(feature_map)\n",
    "    \n",
    "    # Log the model to MLflow and register the model to Unity Catalog\n",
    "    # All model metrics and parameters will be available in Unity Catalog\n",
    "    model_info = mlflow.xgboost.log_model(\n",
    "        xgb_model=reg,\n",
    "        name=\"xgboost_regression_model\",\n",
    "        input_example=X.iloc[[0]],\n",
    "        signature=signature,\n",
    "        registered_model_name=\"jacek_laskowski.mlflow.xgboost_regression_model\",\n",
    "    )\n",
    "\n",
    "    # Log metrics to the run and model\n",
    "    mlflow.log_metric(\"train_rmse\", final_train_rmse)\n",
    "    mlflow.log_metric(\"test_rmse\", final_test_rmse)\n",
    "    \n",
    "    # Log feature analysis plots\n",
    "    # Plots are saved as artifacts in MLflow\n",
    "    mlflow.log_figure(dist_plot, \"feature_distributions.png\")\n",
    "    mlflow.log_figure(corr_plot, \"correlation_heatmap.png\")\n",
    "    mlflow.log_figure(scatter_plot, \"feature_target_relationships.png\")\n",
    "    mlflow.log_figure(pairwise_plot, \"pairwise_relationships.png\")\n",
    "    mlflow.log_figure(outlier_plot, \"outlier_detection.png\")\n",
    "    mlflow.log_figure(residual_plot, \"feature_boxplots_by_target.png\")\n",
    "        \n",
    "    # Plot feature importance\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    xgb.plot_importance(reg, ax=ax, importance_type='gain')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "\n",
    "    mlflow.log_figure(fig, \"feature_importance.png\")\n",
    "\n",
    "    # Run MLflow evaluation to generate additional metrics without having to implement them\n",
    "    mlflow.models.evaluate(\n",
    "        model=model_info.model_uri, \n",
    "        data=evaluation_data, \n",
    "        targets=\"label\", \n",
    "        model_type=\"regressor\", \n",
    "        evaluator_config={\"metric_prefix\": \"mlflow_evaluation_\"},\n",
    "    )\n",
    "    \n",
    "    print(f\"Model logged: {model_info.model_uri}\")\n",
    "    print(f\"Train RMSE: {final_train_rmse:.4f}\")\n",
    "    print(f\"Test RMSE: {final_test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07dc9530-30dd-4deb-b821-30f0c38ff4c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "5. Hyperparameter tuning"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import xgboost as xgb\n",
    "\n",
    "# Generate training and validation data\n",
    "n_samples = 2000\n",
    "n_features = 10\n",
    "\n",
    "X, y = create_regression_data(n_samples=n_samples, n_features=n_features, nonlinear=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the evaluation data\n",
    "evaluation_data = X_test.copy()\n",
    "evaluation_data[\"label\"] = y_test\n",
    "\n",
    "reg.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "reg = xgb.XGBRegressor(\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='rmse',\n",
    ")\n",
    "\n",
    "# The objective function defines the search space for the key hyperparameters of the XGBRegressor algorithm.\n",
    "# Optuna dynamically samples these values, so that each trial tests a different combination of parameters.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\"\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(param)\n",
    "        regressor = xgb.XGBRegressor(**param)\n",
    "        regressor.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "        preds = regressor.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "    # Store the model in the trial's `user attributes`\n",
    "    trial.set_user_attr(\"model\", regressor)\n",
    "    return rmse\n",
    "\n",
    "jacek_n_trials = 20\n",
    "\n",
    "# In the parent run, save the best iteration from the hyperparameter tuning execution\n",
    "with mlflow.start_run():\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=jacek_n_trials)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_model = best_trial.user_attrs[\"model\"]\n",
    "\n",
    "    mlflow.log_metric(\"best_rmse\", best_trial.value)\n",
    "    mlflow.log_params(best_trial.params)\n",
    "\n",
    "    signature = infer_signature(X_train, best_model.predict(X_test))\n",
    "\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=best_model,     \n",
    "        name=\"xgboostoptuna\",\n",
    "        input_example=X_train.iloc[[0]],\n",
    "        signature=signature,\n",
    "        model_format=\"ubj\",\n",
    "        registered_model_name=\"jacek_laskowski.mlflow.xgboostoptuna\",\n",
    "    )\n",
    "\n",
    "    mlflow.models.evaluate(\n",
    "        model=model_info.model_uri, \n",
    "        data=evaluation_data, \n",
    "        targets=\"label\", \n",
    "        model_type=\"regressor\", \n",
    "        evaluator_config={\"metric_prefix\": \"mlflow_evaluation_\"},\n",
    "    )\n",
    "\n",
    "    dist_plot = plot_feature_distributions(X_train, y_train)\n",
    "    corr_plot = plot_correlation_heatmap(X_train, y_train)\n",
    "    scatter_plot = plot_feature_target_relationships(X_train, y_train)\n",
    "\n",
    "    # Select a few interesting features for the pairwise plot\n",
    "    # Choose features with highest correlation with target\n",
    "    corr_with_target = X_train.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "    top_features = corr_with_target.head(4).index.tolist()\n",
    "    pairwise_plot = plot_pairwise_relationships(X, y, top_features)\n",
    "\n",
    "    # Log the plots associated with the parent run only\n",
    "    mlflow.log_figure(dist_plot, \"feature_distributions.png\")\n",
    "    mlflow.log_figure(corr_plot, \"correlation_heatmap.png\")\n",
    "    mlflow.log_figure(scatter_plot, \"feature_target_relationships.png\")\n",
    "    mlflow.log_figure(pairwise_plot, \"pairwise_relationships.png\")\n",
    "    mlflow.log_figure(outlier_plot, \"outlier_detection.png\")\n",
    "    mlflow.log_figure(residual_plot, \"feature_boxplots_by_target.png\")\n",
    "        \n",
    "    # Plot feature importance of the best model only\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    xgb.plot_importance(best_model, ax=ax, importance_type='gain')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "\n",
    "    mlflow.log_figure(fig, \"feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c140a0d9-6cdb-41f6-9c86-3858e5eb0545",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "6. Assign a human-readable alias"
    }
   },
   "outputs": [],
   "source": [
    "# Use the `MlflowClient` to access metadata, artifacts, and information about models that are tracked or registered to the model registry.\n",
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Set the alias on the desired version. This example uses version 1.\n",
    "client.set_registered_model_alias(\"jacek_laskowski.mlflow.xgboostoptuna\", \"best\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5d8b28-57db-442c-aec3-387c43849d7d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "7. Pre-deployment validation"
    }
   },
   "outputs": [],
   "source": [
    "model_uri = \"models:/jacek_laskowski.mlflow.xgboostoptuna@best\"\n",
    "\n",
    "mlflow.models.predict(model_uri=model_uri, input_data=X_train, env_manager=\"uv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f539938-cb5b-446b-907e-a01eca17c97d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "8. Load the registered model and make predictions"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model and use the model to make predictions locally\n",
    "loaded_registered_model = mlflow.pyfunc.load_model(model_uri=model_uri)\n",
    "\n",
    "loaded_registered_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7012504f-33bd-4d86-8e2c-8a690a425698",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "9. Batch prediction using Spark UDF in MLflow"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the training data into a Spark DataFrame.\n",
    "X_spark = spark.createDataFrame(X_train)\n",
    "\n",
    "# Create a Spark UDF to apply the model to the Spark DataFrame.\n",
    "# Note that `model_uri` is defined based on a model alias, ensuring that you always load the current, approved version.\n",
    "udf = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri=model_uri,\n",
    ")\n",
    "\n",
    "# Apply the Spark UDF to the DataFrame. This performs batch predictions across all rows in a distributed manner. \n",
    "X_spark = X_spark.withColumn(\"prediction\", udf(*X_train.columns))\n",
    "\n",
    "# Display the resulting DataFrame. \n",
    "display(X_spark)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Copy of XGBoost MLflow 3 Tutorial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
