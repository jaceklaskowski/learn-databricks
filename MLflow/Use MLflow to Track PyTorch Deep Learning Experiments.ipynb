{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd8bf3fe-94e9-439d-8d8f-54b943ac82fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Use MLflow to Track PyTorch Deep Learning Experiments\n",
    "\n",
    "The source: [Deep Learning Quickstart](https://mlflow.org/docs/latest/ml/getting-started/deep-learning/)\n",
    "\n",
    "In this tutorial, we demonstrate how to use MLflow to track deep learning experiments with PyTorch:\n",
    "\n",
    "1. Save checkpoints with metrics.\n",
    "1. Visualize the loss curve during training.\n",
    "1. Monitor system metrics such as GPU utilization, memory footprint, disk usage, network, etc.\n",
    "1. Record hyperparameters and optimizer settings.\n",
    "1. Snapshot library versions for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20c139a2-88fc-4cb6-b876-ecfdaf9b0635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Install MLflow and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3487be-c928-4c02-853a-b351f89ab5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU mlflow torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d7b247-a30e-4d3c-b0fe-f1e6edd4722f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438cb611-b7b2-42f4-8e45-7c96bc4626a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1c7653-69b3-4f3e-aef4-78134e6f192e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 1: Create a new experiment\n",
    "\n",
    "Well...it is created automatically when this notebook runs in Databricks.\n",
    "\n",
    "The title of the notebook is the name of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938e51d8-c56d-41ce-af9d-3e346ad495c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# IMPORTANT: Enable system metrics monitoring\n",
    "mlflow.config.enable_system_metrics_logging()\n",
    "mlflow.config.set_system_metrics_sampling_interval(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d553415-a329-44a4-ace8-f73e0bf4e297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 2: Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2853c893-88ec-4c8b-8f46-461e3cc9228d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and prepare data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    \"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\"data\", train=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88f1b365-839b-406c-aac7-cbabd4255eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3: Define the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8dee7c8-3e46-40eb-b2a0-4db6d2ce99e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afaef48-aa51-4f13-afc5-7a6b6998970d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "params = {\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 64,\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"model_type\": \"MLP\",\n",
    "    \"hidden_units\": [512, 512],\n",
    "}\n",
    "\n",
    "# Define optimizer and loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=params[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0fc519a-8d42-423d-b62b-61faafc31d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23548aa2-a9aa-4b63-8c7c-bb2e48ea3549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    # Log training parameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            # Log batch metrics (every 100 batches)\n",
    "            if batch_idx % 100 == 0:\n",
    "                batch_loss = train_loss / (batch_idx + 1)\n",
    "                batch_acc = 100.0 * correct / total\n",
    "                mlflow.log_metrics(\n",
    "                    {\"batch_loss\": batch_loss, \"batch_accuracy\": batch_acc},\n",
    "                    step=epoch * len(train_loader) + batch_idx,\n",
    "                )\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = train_loss / len(train_loader)\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        # Calculate and log epoch validation metrics\n",
    "        val_loss = val_loss / len(test_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        # Log epoch metrics\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                \"train_loss\": epoch_loss,\n",
    "                \"train_accuracy\": epoch_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_acc,\n",
    "            },\n",
    "            step=epoch,\n",
    "        )\n",
    "        # Log checkpoint at the end of each epoch\n",
    "        mlflow.pytorch.log_model(model, name=f\"checkpoint_{epoch}\")\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{params['epochs']}, \"\n",
    "            f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "    # Log the final trained model\n",
    "    model_info = mlflow.pytorch.log_model(model, name=\"final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96cf5030-08b8-44ce-b58d-9f36a4dfa5a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 5: View the training results in the MLflow UI\n",
    "\n",
    "Review the Databricks UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0309438-936b-43d3-8f11-00262642730e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 6: Load back the model and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73512e6-9c99-4761-a480-8cfc8527f170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the final model\n",
    "model = mlflow.pytorch.load_model(model_info.artifact_path)\n",
    "# or load a checkpoint\n",
    "# model = mlflow.pytorch.load_model(\"runs:/<run_id>/checkpoint_<epoch>\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Resume the previous run to log test metrics\n",
    "with mlflow.start_run(run_id=run.info.run_id) as run:\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_correct, test_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        test_total += target.size(0)\n",
    "        test_correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    # Calculate and log final test metrics\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100.0 * test_correct / test_total\n",
    "\n",
    "    mlflow.log_metrics({\"test_loss\": test_loss, \"test_accuracy\": test_acc})\n",
    "    print(f\"Final Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Use MLflow to Track PyTorch Deep Learning Experiments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
