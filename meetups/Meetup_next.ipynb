{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a46b538-5343-4bc0-b5fc-97449b4aa283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Quality in Databricks Workflows (jobs) with Pydantic\n",
    "\n",
    "> ‚ö†Ô∏è This is a draft of the agenda of the future meetup\n",
    "\n",
    "Thursday, February 20, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd081a26-c5e2-444a-875d-64c485221a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "‚û°Ô∏è [Meetup Announcement](https://www.meetup.com/warsaw-data-engineering/events/306200574/)\n",
    "\n",
    "This meetup is a continuation of the two past events.\n",
    "\n",
    "W poprzednim odcinku:\n",
    "\n",
    "1. Projekt Databricks zarzƒÖdzany przez Databricks Asset Bundles (DAB)\n",
    "1. Pierwszy projekt z pydantic (libka w Pythonie), a drugi to \"hello world\" Databricks Asset Bundle project z przyk≈Çadowym job'em.\n",
    "\n",
    "Agenda:\n",
    "\n",
    "1. **10 minut** Og≈Çoszenia. Czas na szalone pomys≈Çy na przysz≈Çe meetupy üëª\n",
    "    * News (new versions, new features, etc.)\n",
    "1. **55 minut** Live coding session, a w nim:\n",
    "    * Przypomnimy sobie osiƒÖgniƒôcie poprzednich meetup√≥w: Databricks Asset Bundle (DAB) z Databricks job z pojedynczym notebookiem z libkƒÖ w Pythonie z Pydantic. Korzystamy z uv do zarzƒÖdzania libkƒÖ w Pythonie.\n",
    "    * G≈Ç√≥wny cel meetupu: Stworzymy UDFa do walidacji rekord√≥w, kt√≥rego \"uzbroimy\" w pydantic'a. To mia≈Ç byƒá g≈Ç√≥wny cel poprzedniego meetupu, ale nie wysz≈Ço i bƒôdzie ponownie ü§∑‚Äç‚ôÇÔ∏è\n",
    "1. **10 minut** Q&A i zbieranie pomys≈Ç√≥w na kolejne edycje\n",
    "\n",
    "Ca≈Çkowity czas trwania meetupu: **1h 15min**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0099d7a7-078c-4a57-8671-dcd404040811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Event Question\n",
    "\n",
    "O czym chcia≈Ç(a)by≈õ us≈Çyszeƒá podczas meetupu? Rzuƒá ciekawym pomys≈Çem na kolejne edycje üôè\n",
    "\n",
    "1. staram siƒô nadarzyƒá za tym co Jacek m√≥wi i czego≈õ siƒô dowiedzieƒá\n",
    "1. framework w Python - best practices\n",
    "1. DAB\n",
    "1. continue exploring quality with dqx or dlt publish to different schemas as good standard for medalion\n",
    "1. Plan rozwoju, do≈õwiadczenia zawodowe wymiataczy technologicznych\n",
    "1. Jakie≈õ zaawansowane data quality w DBR; mo≈ºe jaka≈õ analiza wykorzystania narzƒôdzi typu Polars/DuckDB dla jedno-node‚Äôowych klastr√≥w?\n",
    "1. Je≈ºeli uda mi siƒô do≈ÇƒÖczyƒá, to bƒôdzie fajnie pos≈Çuchaƒá dalszej cze≈õci poprzedniego meetup'u :)\n",
    "1. Pod≈ÇƒÖczenie Master data w transformacjach Databricks\n",
    "1. Pydantic\n",
    "1. everything about databricks\n",
    "1. Chcƒô rozwijaƒá swoje umiejƒôtno≈õci w databricks a ta seria spotka≈Ñ to co≈õ czego szuka≈Çem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "811aaee2-66d6-4833-84d3-c5971e3359ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì¢ News\n",
    "\n",
    "Things worth watching out for..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a32e9654-31e0-4afa-b153-320773a495ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## New Versions\n",
    "\n",
    "What has changed in the tooling space we keep an eye on since we last met?\n",
    "\n",
    "* Databricks CLI\n",
    "* [uv 0.5.29](https://github.com/astral-sh/uv/releases/tag/0.5.29)\n",
    "* [MLflow 2.20.2](https://github.com/mlflow/mlflow/releases/tag/v2.20.2)\n",
    "    * released this 2 days ago with 176 commits to master since this release ü§®\n",
    "* [awscli 2.24.0](https://github.com/aws/aws-cli/releases/tag/2.24.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4413b3e-c3ae-435f-bd5e-daf457ca6b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## DQX by Databricks Labs\n",
    "\n",
    "https://github.com/databrickslabs/dqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b8fc88a-0347-43f6-9281-1c5337e73779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# üëÄ In the spotlight: `uv`\n",
    "\n",
    "It is one of the regular sections in our schedule until we run out of...interest to dig deeper and learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0117e3a-3e05-49f5-9712-f63db1b35494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## uv init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07789504-d4f2-4094-bc23-c3d3c4574b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The very recent change was to add `--bare` option to `uv init`. Why is this important?\n",
    "\n",
    "<br>\n",
    "\n",
    "```py\n",
    "uv init --bare\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "421ed372-d597-4f95-a551-d865c9160995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "* [Working on projects](https://docs.astral.sh/uv/guides/projects/)\n",
    "* [Creating projects](https://docs.astral.sh/uv/concepts/projects/init/)\n",
    "* [uv init](https://docs.astral.sh/uv/reference/cli/#uv-init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a55b51-9ba1-42a6-a0a1-8952fc125623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Live Coding Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "801bc20d-cb58-4564-9d8e-74489c161f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ‚úÖ Create Databricks Project\n",
    "\n",
    "Databricks Asset Bundles (DAB) enters the scene üé¨\n",
    "\n",
    "`databricks bundle init default-python`\n",
    "\n",
    "* Name: `pydantic_workflow` (in `demo` directory)\n",
    "* Python included\n",
    "* No DLT pipelines\n",
    "\n",
    "Learn more:\n",
    "\n",
    "1. [Databricks Asset Bundles development](https://docs.databricks.com/en/dev-tools/bundles/work-tasks.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "641fc39e-6edb-4b9b-829d-1c7923d3f080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### databricks.yml and the resources\n",
    "\n",
    "Review the following:\n",
    "\n",
    "1. `databricks.yml`\n",
    "    * Make sure that `workspace/host` section points at the proper Databricks workspace\n",
    "1. `resources/*.yml` (included in `databricks.yml`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "942e4586-ceb3-40d2-8afb-afb056772d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Clean Up\n",
    "\n",
    "> ‚ö†Ô∏è NOTE\n",
    ">\n",
    "> This step is not required at such an early stage of Databricks project's development.\n",
    "> You may skip it.\n",
    "\n",
    "Remove the following (unnecessary) files and directories:\n",
    "\n",
    "1. `rm pytest.ini requirements-dev.txt setup.py`\n",
    "1. `rm -rf fixtures scratch dist`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b887cff7-674e-4d68-b078-cf83205335f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Validate Bundle\n",
    "\n",
    "`databricks bundle validate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce18b3ab-d020-4999-bcbc-8c342fda0379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "```\n",
    "Name: pydantic_workflow\n",
    "Target: dev\n",
    "Workspace:\n",
    "  Host: https://curriculum-dev.cloud.databricks.com\n",
    "  User: jacek@japila.pl\n",
    "  Path: /Workspace/Users/jacek@japila.pl/.bundle/pydantic_workflow/dev\n",
    "\n",
    "Validation OK!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "293dedde-9720-4e40-b950-731233b9e4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Deploy Bundle\n",
    "\n",
    "`databricks bundle deploy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7ba968c-5c86-4e06-9187-75f73ae1cf25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Unless you removed the project sources, you should see the following logs while deploying the bundle:\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "‚ùØ databricks bundle deploy\n",
    "Building uv_workflows...\n",
    "Uploading uv_workflows-0.0.1+20250109.152923-py3-none-any.whl...\n",
    "...\n",
    "```\n",
    "\n",
    "This `Building` step is triggered because `setup.py` is in the main directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab1c490b-8d5f-4c1d-8c67-eb811930bd95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "```\n",
    "‚ùØ databricks bundle deploy\n",
    "Error: no files match pattern: dist/*.whl\n",
    "  at resources.jobs.pydantic_workflow_job.tasks[1].libraries[0].whl\n",
    "  in resources/pydantic_workflow.job.yml:35:15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e50e0479-76e5-469e-a7df-127a86d85151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "That's expected since there's no Python wheel to be deployed.\n",
    "\n",
    "Referenced in `resources/pydantic_workflow.job.yml`\n",
    "\n",
    "An easy fix is to comment out `main_task` task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b993854-d9f1-4ffa-9527-abbf05780251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ databricks bundle deploy\n",
    "Uploading bundle files to /Workspace/Users/jacek@japila.pl/.bundle/pydantic_workflow/dev/files...\n",
    "Deploying resources...\n",
    "Updating deployment state...\n",
    "Deployment complete!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4402b464-0834-4c14-b4ed-1d0ab7e2311c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Run Job\n",
    "\n",
    "`databricks bundle run pydantic_workflow_job`\n",
    "\n",
    "> ‚ö†Ô∏è Hint\n",
    ">\n",
    "> Use auto-completion while typing `databricks bundle` commands (incl. the names of resources).\n",
    "\n",
    "It should work just fine.\n",
    "\n",
    "The notebook uses the Python code directly (they're in the same directory). All seems OK. Why bother with `uv`?! ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bddaa8f4-3836-4365-b8e2-ef653ecfd254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PERMISSION_DENIED: You are not authorized to create clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e6b0403-4f68-4586-9a02-c02e6093b2ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ databricks bundle run pydantic_workflow_job\n",
    "Run URL: https://curriculum-dev.cloud.databricks.com/?o=3551974319838082#job/52151941639258/run/148305795734637\n",
    "\n",
    "2025-02-16 19:37:26 \"[dev jacek] pydantic_workflow_job\" RUNNING\n",
    "2025-02-16 19:37:28 \"[dev jacek] pydantic_workflow_job\" INTERNAL_ERROR FAILED Task notebook_task failed with message: Unexpected user error while preparing the cluster for the job. Cause: PERMISSION_DENIED: You are not authorized to create clusters. Please contact your administrator. This caused all downstream tasks to get skipped.\n",
    "Task notebook_task FAILED:\n",
    "run failed with error message\n",
    " Unexpected user error while preparing the cluster for the job. Cause: PERMISSION_DENIED: You are not authorized to create clusters. Please contact your administrator.\n",
    "\n",
    "\n",
    "Error: Task notebook_task failed!\n",
    "Error:\n",
    "run failed with error message\n",
    " Unexpected user error while preparing the cluster for the job. Cause: PERMISSION_DENIED: You are not authorized to create clusters. Please contact your administrator.\n",
    "Trace:\n",
    "\n",
    "Error: failed to reach TERMINATED or SKIPPED, got INTERNAL_ERROR: Task notebook_task failed with message: Unexpected user error while preparing the cluster for the job. Cause: PERMISSION_DENIED: You are not authorized to create clusters. Please contact your administrator. This caused all downstream tasks to get skipped.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c40041d8-a53d-4ad2-8287-4c76176801df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "If you run into `PERMISSION_DENIED: You are not authorized to create clusters` (shown above), replace `job_cluster_key` in the job definition file with `existing_cluster_id: [server_id]`.\n",
    "\n",
    "Or simply talk to the Databricks admins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9281fc53-4e80-455a-bdbe-2b47d6d8140f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ databricks bundle run pydantic_workflow_job\n",
    "Run URL: https://curriculum-dev.cloud.databricks.com/?o=3551974319838082#job/52151941639258/run/379631815214233\n",
    "\n",
    "2025-02-16 19:43:47 \"[dev jacek] pydantic_workflow_job\" RUNNING\n",
    "2025-02-16 19:49:48 \"[dev jacek] pydantic_workflow_job\" TERMINATED SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a5ac90e-f887-48d5-9bd4-ed2fc7883827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Motivation\n",
    "\n",
    "(The Leading Idea of This Meetup Series)\n",
    "\n",
    "Let's pause for a moment and try to answer the following question:\n",
    "\n",
    "> The bundle works (deploys and runs), so why use `uv`, `poetry`, or any other Python build tool?! What are we missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df0216e-2bc2-46f4-87c7-009540865f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Possible answers:\n",
    "\n",
    "1. We want to execute tests before deployment (and other CI/CD-like management tasks to be executed locally)\n",
    "1. More importantly, this [python_wheel_task](https://docs.databricks.com/api/workspace/jobs/create#tasks-python_wheel_task) in `pydantic_workflow_job` definition could be a separate project (with its own lifecycle, independent of the DAB project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72e21d42-6507-4f3c-b016-2cdc269d80aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ‚úÖ Create uv Project\n",
    "\n",
    "`uv init --bare`\n",
    "\n",
    "[Develop a Python wheel file using Databricks Asset Bundles](https://docs.databricks.com/en/dev-tools/bundles/python-wheel.html) (esp. [Step 4: Update the project‚Äôs bundle to use Poetry](https://docs.databricks.com/en/dev-tools/bundles/python-wheel.html))\n",
    "\n",
    "> By default, the bundle template specifies building the Python wheel file using `setuptools` along with the files `setup.py` and `requirements-dev.txt`.\n",
    "\n",
    "[Databricks Asset Bundle configuration](https://docs.databricks.com/en/dev-tools/bundles/settings.html) (esp. [artifacts mapping](https://docs.databricks.com/en/dev-tools/bundles/settings.html#artifacts))\n",
    "\n",
    "> The top-level artifacts mapping specifies one or more artifacts that are automatically built during bundle deployments and can be used later in bundle runs.\n",
    "\n",
    "Learn more:\n",
    "\n",
    "1. [Working on projects](https://docs.astral.sh/uv/guides/projects/)\n",
    "1. [Building your package](https://docs.astral.sh/uv/guides/publish/#building-your-package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3881264e-8746-4daa-9da2-2d0604bd44a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ uv init --bare\n",
    "Initialized project `pydantic-workflow`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "036758b6-9647-4e57-91ba-18cbb8613f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### pyproject.toml\n",
    "\n",
    "Review `pyproject.toml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "681a72ea-ac11-4615-a951-7a0234b1eb3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Build Python wheel\n",
    "\n",
    "`uv build --wheel`\n",
    "\n",
    "> **build**  Build Python packages into source distributions and wheels\n",
    "\n",
    "`uv build --help` (esp. `uv build --wheel`)\n",
    "\n",
    "Learn more in [Building your package](https://docs.astral.sh/uv/guides/publish/#building-your-package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5f985a2-f488-4033-a96e-a2fd0a059ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ uv build --wheel\n",
    "Building wheel...\n",
    "running egg_info\n",
    "creating src/pydantic_workflow.egg-info\n",
    "writing src/pydantic_workflow.egg-info/PKG-INFO\n",
    "writing dependency_links to src/pydantic_workflow.egg-info/dependency_links.txt\n",
    "writing top-level names to src/pydantic_workflow.egg-info/top_level.txt\n",
    "writing manifest file 'src/pydantic_workflow.egg-info/SOURCES.txt'\n",
    "reading manifest file 'src/pydantic_workflow.egg-info/SOURCES.txt'\n",
    "writing manifest file 'src/pydantic_workflow.egg-info/SOURCES.txt'\n",
    "running bdist_wheel\n",
    "running build\n",
    "running build_py\n",
    "creating build/lib/pydantic_workflow\n",
    "copying src/pydantic_workflow/__init__.py -> build/lib/pydantic_workflow\n",
    "copying src/pydantic_workflow/main.py -> build/lib/pydantic_workflow\n",
    "running egg_info\n",
    "writing src/pydantic_workflow.egg-info/PKG-INFO\n",
    "writing dependency_links to src/pydantic_workflow.egg-info/dependency_links.txt\n",
    "writing top-level names to src/pydantic_workflow.egg-info/top_level.txt\n",
    "reading manifest file 'src/pydantic_workflow.egg-info/SOURCES.txt'\n",
    "writing manifest file 'src/pydantic_workflow.egg-info/SOURCES.txt'\n",
    "installing to build/bdist.macosx-10.9-x86_64/wheel\n",
    "running install\n",
    "running install_lib\n",
    "creating build/bdist.macosx-10.9-x86_64/wheel\n",
    "creating build/bdist.macosx-10.9-x86_64/wheel/pydantic_workflow\n",
    "copying build/lib/pydantic_workflow/__init__.py -> build/bdist.macosx-10.9-x86_64/wheel/./pydantic_workflow\n",
    "copying build/lib/pydantic_workflow/main.py -> build/bdist.macosx-10.9-x86_64/wheel/./pydantic_workflow\n",
    "running install_egg_info\n",
    "Copying src/pydantic_workflow.egg-info to build/bdist.macosx-10.9-x86_64/wheel/./pydantic_workflow-0.1.0-py3.11.egg-info\n",
    "running install_scripts\n",
    "creating build/bdist.macosx-10.9-x86_64/wheel/pydantic_workflow-0.1.0.dist-info/WHEEL\n",
    "creating '/Users/jacek/dev/learn-databricks/demo/pydantic_workflow/dist/.tmp-ssvefrd_/pydantic_workflow-0.1.0-py3-none-any.whl' and adding 'build/bdist.macosx-10.9-x86_64/wheel' to it\n",
    "adding 'pydantic_workflow/__init__.py'\n",
    "adding 'pydantic_workflow/main.py'\n",
    "adding 'pydantic_workflow-0.1.0.dist-info/METADATA'\n",
    "adding 'pydantic_workflow-0.1.0.dist-info/WHEEL'\n",
    "adding 'pydantic_workflow-0.1.0.dist-info/top_level.txt'\n",
    "adding 'pydantic_workflow-0.1.0.dist-info/RECORD'\n",
    "removing build/bdist.macosx-10.9-x86_64/wheel\n",
    "Successfully built dist/pydantic_workflow-0.1.0-py3-none-any.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e04b796-7156-43e7-9759-f15efa13a7c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Integrate DAB and uv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a4b5523-25f7-4c68-8b2b-7fcfdd39cf9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Remember the error? Time to fix it in a more professional way üòâ Before, we simply commented out the task that uses the wheel.\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "‚ùØ databricks bundle deploy\n",
    "Error: no files match pattern: dist/*.whl\n",
    "  at resources.jobs.pydantic_workflow_job.tasks[1].libraries[0].whl\n",
    "  in resources/pydantic_workflow.job.yml:35:15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f443bd8-acbb-41aa-8439-2886c1b1fdad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### databricks.yml and artifacts\n",
    "\n",
    "Add the following `artifacts` section to `databricks.yml`.\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "artifacts:\n",
    "  pydantic_workflow_wheel:\n",
    "    type: whl\n",
    "    build: uv build --wheel\n",
    "    path: .\n",
    "```\n",
    "\n",
    "Learn more:\n",
    "\n",
    "1. [artifacts](https://docs.databricks.com/en/dev-tools/bundles/settings.html#artifacts) mapping\n",
    "1. [Databricks Asset Bundle configuration](https://docs.databricks.com/en/dev-tools/bundles/settings.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdea2700-3e54-4e17-b84d-266555403a64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### resources/pydantic_workflow.job.yml\n",
    "\n",
    "Uncomment `main_task` in `resources/pydantic_workflow.job.yml` (that uses `libraries` with the wheel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c32035f6-1240-45b8-aba7-f638c367c4b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Redeploy DAB\n",
    "\n",
    "`databricks bundle deploy` to re-deploy the bundle. This time the lib's built by `uv` ‚ù§Ô∏è\n",
    "\n",
    "With the changes, you should see `Building pydantic_workflow_wheel...` message while `databricks bundle deploy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b7d119c-4f4a-4173-9705-e87016a14dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ databricks bundle deploy\n",
    "Building pydantic_workflow_wheel...\n",
    "Uploading pydantic_workflow-0.1.0-py3-none-any.whl...\n",
    "Uploading bundle files to /Workspace/Users/jacek@japila.pl/.bundle/pydantic_workflow/dev/files...\n",
    "Deploying resources...\n",
    "Updating deployment state...\n",
    "Deployment complete!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef27514-9cdd-4060-a7cc-b29f5f4e5ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Run Job\n",
    "\n",
    "`databricks bundle run pydantic_workflow_job`\n",
    "\n",
    "There should be two tasks executed properly (incl. `main_task` with the uv-managed Python wheel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87536f8-e6dd-4c13-935e-876a9a1c2bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ databricks bundle run pydantic_workflow_job\n",
    "Run URL: https://curriculum-dev.cloud.databricks.com/?o=3551974319838082#job/52151941639258/run/1079588996512936\n",
    "\n",
    "2025-02-16 20:43:50 \"[dev jacek] pydantic_workflow_job\" RUNNING\n",
    "2025-02-16 20:44:23 \"[dev jacek] pydantic_workflow_job\" TERMINATED SUCCESS\n",
    "Output:\n",
    "=======\n",
    "Task notebook_task:\n",
    "\n",
    "=======\n",
    "Task main_task:\n",
    "/databricks/python/lib/python3.12/site-packages/databricks/sdk/service/jobs.py:60: SyntaxWarning: invalid escape sequence '\\.'\n",
    "  \"\"\"The sequence number of this run attempt for a triggered job run. The initial attempt of a run\n",
    "/databricks/python/lib/python3.12/site-packages/databricks/sdk/service/jobs.py:2570: SyntaxWarning: invalid escape sequence '\\.'\n",
    "  \"\"\"The sequence number of this run attempt for a triggered job run. The initial attempt of a run\n",
    "/databricks/python/lib/python3.12/site-packages/databricks/sdk/service/jobs.py:3431: SyntaxWarning: invalid escape sequence '\\.'\n",
    "  \"\"\"The sequence number of this run attempt for a triggered job run. The initial attempt of a run\n",
    "\n",
    "\n",
    "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
    "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|pickup_zip|dropoff_zip|\n",
    "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
    "| 2016-02-13 21:47:53|  2016-02-13 21:57:15|          1.4|        8.0|     10103|      10110|\n",
    "| 2016-02-13 18:29:09|  2016-02-13 18:37:23|         1.31|        7.5|     10023|      10023|\n",
    "| 2016-02-06 19:40:58|  2016-02-06 19:52:32|          1.8|        9.5|     10001|      10018|\n",
    "| 2016-02-12 19:06:43|  2016-02-12 19:20:54|          2.3|       11.5|     10044|      10111|\n",
    "| 2016-02-23 10:27:56|  2016-02-23 10:58:33|          2.6|       18.5|     10199|      10022|\n",
    "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51b4ab64-7e6f-4b23-ae74-de12ab62d35d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Review\n",
    "\n",
    "Open up the workspace and review `main_task` definition. There should be our uv-built wheel under **Dependent libraries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1867a032-17c3-4ef1-b392-d73804b999f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "![](./pydantic_workflow_main_task_python_wheel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb21924e-d3a8-4128-943d-01c5a4e31b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "> ‚ö†Ô∏è FIXME\n",
    ">\n",
    "> The following image is outdated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f5a4aea-fbd4-4897-80ea-0770b1b0490c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "![](./uv_workflow_job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838768bb-1382-4b99-b71b-c4efd9ca5211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### (Optional) Destroy Bundle\n",
    "\n",
    "This is an optional step to remove everything in the Databricks workspace so we can start afresh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be406a28-5b05-483a-85cc-11bd1a364532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ databricks bundle destroy --auto-approve\n",
    "The following resources will be deleted:\n",
    "  delete job pydantic_workflow_job\n",
    "\n",
    "All files and directories at the following location will be deleted: /Workspace/Users/jacek@japila.pl/.bundle/pydantic_workflow/dev\n",
    "\n",
    "Deleting files...\n",
    "Destroy complete!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49e21f76-24e7-415f-b1b1-96d960eded45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ‚úÖ Add Pydantic to the mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7680a28-7db7-46b9-972b-01e585485956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Pin Python\n",
    "\n",
    "`uv python pin`\n",
    "\n",
    "Pin the Python version to match Databricks Runtime's Python (to avoid errors due to Python mis-configuration).\n",
    "\n",
    "> ‚ö†Ô∏è Note\n",
    ">\n",
    "> [Databricks Runtime 16.2](https://docs.databricks.com/en/release-notes/runtime/16.2.html#system-environment) runs with Python 3.12.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412be30b-25c8-4b4c-9a91-f1148a863ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Unless done already, install the desired Python version with `uv python install`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4463281f-b534-468e-9cea-b156f794ade7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ uv python install 3.12.3\n",
    "Installed Python 3.12.3 in 9.11s\n",
    " + cpython-3.12.3-macos-x86_64-none\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eb92e7b-2d10-41eb-8df4-db45a2fadbb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ uv python pin 3.12.3\n",
    "Pinned `.python-version` to `3.12.3`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f54740-7805-4be7-a0c2-7166b38b9f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add Pydantic Dependency\n",
    "\n",
    "`uv add`\n",
    "\n",
    "Pydantic is the main dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8660543-b685-4959-a835-d5989505ac86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "```\n",
    "‚ùØ uv add pydantic\n",
    "Resolved 5 packages in 346ms\n",
    "Installed 4 packages in 7ms\n",
    " + annotated-types==0.7.0\n",
    " + pydantic==2.10.6\n",
    " + pydantic-core==2.27.2\n",
    " + typing-extensions==4.12.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e7ccdb4-011a-40a8-91a7-22e797438143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Review `pyproject.toml`.\n",
    "\n",
    "There should be the following new section:\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "dependencies = [\n",
    "    \"pydantic>=2.10.6\",\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91e60790-6fc8-4795-a5e4-46c75bbb7e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build Project\n",
    "\n",
    "`uv build --wheel` for a test run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7963087e-b9be-4c80-8a6d-318ebc355d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Write Tests\n",
    "\n",
    "There's no better way to validate our code than the accompanying tests.\n",
    "\n",
    "I'd not be surprised if you always start a project with tests first (see [Test-driven development](https://en.wikipedia.org/wiki/Test-driven_development))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18fe5a86-8f62-4ce3-9aec-6d18548b97b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Add Test Dependencies\n",
    "\n",
    "Add a couple of development (test) dependencies to the project with `uv add --dev`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41d1583d-2cc9-4a96-9c64-dc8db0046172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ uv add --dev pyspark pytest\n",
    "Resolved 12 packages in 17.60s\n",
    "      Built pyspark==3.5.4\n",
    "Prepared 6 packages in 10.33s\n",
    "Installed 6 packages in 24ms\n",
    " + iniconfig==2.0.0\n",
    " + packaging==24.2\n",
    " + pluggy==1.5.0\n",
    " + py4j==0.10.9.7\n",
    " + pyspark==3.5.4\n",
    " + pytest==8.3.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0250521-3641-4008-881c-a7d995b2378f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Review `pyproject.toml`.\n",
    "\n",
    "There should be the following new section:\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "[dependency-groups]\n",
    "dev = [\n",
    "    \"pyspark>=3.5.4\",\n",
    "    \"pytest>=8.3.4\",\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cca802d2-7b54-4be0-9248-6b682dc6415d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Run Tests\n",
    "\n",
    "There are no tests yet, but with `pytest` defined as a dev dependency you should still be able to run `uv run pytest`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117cadf1-247e-438d-af03-56a86a6e1cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "```\n",
    "‚ùØ uv run pytest\n",
    "================================================= test session starts =================================================\n",
    "platform darwin -- Python 3.12.3, pytest-8.3.4, pluggy-1.5.0\n",
    "rootdir: /Users/jacek/dev/learn-databricks/demo/pydantic_workflow\n",
    "configfile: pyproject.toml\n",
    "collected 0 items\n",
    "\n",
    "================================================ no tests ran in 0.00s ================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8752cf0a-8372-4bfa-a792-42bf863b995e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "It works! ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b762e224-3c37-4ddb-897b-65d7b7cff219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test-Driven Development\n",
    "\n",
    "Create `tests` directory with the following `test_trip.py` file.\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "from pydantic_workflow.trip import Trip\n",
    "\n",
    "\n",
    "def test_valid_trip():\n",
    "    Trip(id=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd13ccfe-c81a-48e7-90fb-5b7849f0c7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Execute `uv run pytest` (that should fail as there's no `Trip` class yet and, most likely, the sources live under `src` directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f52ab30d-a489-4ce8-82b4-ba0bfae5c0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ uv run pytest\n",
    "================================================= test session starts =================================================\n",
    "platform darwin -- Python 3.12.3, pytest-8.3.4, pluggy-1.5.0\n",
    "rootdir: /Users/jacek/dev/learn-databricks/demo/pydantic_workflow\n",
    "configfile: pyproject.toml\n",
    "collected 0 items / 1 error\n",
    "\n",
    "======================================================= ERRORS ========================================================\n",
    "_________________________________________ ERROR collecting tests/test_trip.py _________________________________________\n",
    "ImportError while importing test module '/Users/jacek/dev/learn-databricks/demo/pydantic_workflow/tests/test_trip.py'.\n",
    "Hint: make sure your test modules/packages have valid Python names.\n",
    "Traceback:\n",
    "../../../../.local/share/uv/python/cpython-3.12.3-macos-x86_64-none/lib/python3.12/importlib/__init__.py:90: in import_module\n",
    "    return _bootstrap._gcd_import(name[level:], package, level)\n",
    "tests/test_trip.py:1: in <module>\n",
    "    from pydantic_workflow.trip import Trip\n",
    "E   ModuleNotFoundError: No module named 'pydantic_workflow'\n",
    "=============================================== short test summary info ===============================================\n",
    "ERROR tests/test_trip.py\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "================================================== 1 error in 0.06s ===================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e0c4c81-47cf-4039-b860-3a4083668021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ModuleNotFoundError: No module named 'pydantic_workflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1883a7d-f7ad-4d67-b987-73ab685fbf65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "By default, uv assumes the sources are in the main directory (not `src` as Databricks Asset Bundles does).\n",
    "\n",
    "Learn more in [Packaged applications](https://docs.astral.sh/uv/concepts/projects/init/#packaged-applications).\n",
    "\n",
    "Add the following to `pyproject.toml`:\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "[build-system]\n",
    "requires = [\"hatchling\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02b0b8c9-7a9a-4f7b-b85a-5409212a4aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Execute `uv run pytest` that should fail due to `Trip` class missing (`src` directory with the sources is not an issue anymore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4c86773-07d9-4156-82c4-783590ce1ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ uv run pytest\n",
    "      Built pydantic-workflow @ file:///Users/jacek/dev/learn-databricks/demo/pydantic_workflow\n",
    "Installed 1 package in 3ms\n",
    "================================================= test session starts =================================================\n",
    "platform darwin -- Python 3.12.3, pytest-8.3.4, pluggy-1.5.0\n",
    "rootdir: /Users/jacek/dev/learn-databricks/demo/pydantic_workflow\n",
    "configfile: pyproject.toml\n",
    "collected 0 items / 1 error\n",
    "\n",
    "======================================================= ERRORS ========================================================\n",
    "_________________________________________ ERROR collecting tests/test_trip.py _________________________________________\n",
    "ImportError while importing test module '/Users/jacek/dev/learn-databricks/demo/pydantic_workflow/tests/test_trip.py'.\n",
    "Hint: make sure your test modules/packages have valid Python names.\n",
    "Traceback:\n",
    "../../../../.local/share/uv/python/cpython-3.12.3-macos-x86_64-none/lib/python3.12/importlib/__init__.py:90: in import_module\n",
    "    return _bootstrap._gcd_import(name[level:], package, level)\n",
    "tests/test_trip.py:1: in <module>\n",
    "    from pydantic_workflow.trip import Trip\n",
    "E   ModuleNotFoundError: No module named 'pydantic_workflow.trip'\n",
    "=============================================== short test summary info ===============================================\n",
    "ERROR tests/test_trip.py\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "================================================== 1 error in 0.06s ===================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fa822c5-da5d-417e-9bc2-bb7396e6ac62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Trip Pydantic Model\n",
    "\n",
    "`Trip` class will be a Pydantic model.\n",
    "\n",
    "This will be used to validate incoming records from the pre-installed `samples.nyctaxi.trips` delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a54f4e1-959d-45fd-9d89-7a6b55e2cd05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Add the following to `src/pydantic_workflow/trip.py`:\n",
    "\n",
    "<br>\n",
    "\n",
    "```py\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Trip(BaseModel):\n",
    "    id: int\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b7f1a66-c6da-41b2-9c32-aa34d6cde46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Run the tests.\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "‚ùØ uv run pytest\n",
    "================================================= test session starts =================================================\n",
    "platform darwin -- Python 3.12.3, pytest-8.3.4, pluggy-1.5.0\n",
    "rootdir: /Users/jacek/dev/learn-databricks/demo/pydantic_workflow\n",
    "configfile: pyproject.toml\n",
    "collected 1 item\n",
    "\n",
    "tests/test_trip.py .                                                                                            [100%]\n",
    "\n",
    "================================================== 1 passed in 0.89s ==================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3a335c6-7850-41da-adc7-c143848733e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "It works! ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd7a0185-5a36-49ae-af32-0a17b2b8dcac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Use Pydantic Validation\n",
    "\n",
    "Let's extend `Trip` class to match the schema of the trips from `samples.nyctaxi.trips` table and accept records with `pickup_zip` and `dropoff_zip` different.\n",
    "\n",
    "Learn more in [Validators](https://docs.pydantic.dev/latest/concepts/validators/) (particularly [After validators](https://docs.pydantic.dev/latest/concepts/validators/#model-after-validator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c014d32-cd0c-457a-a124-06e099a299a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```py\n",
    "from typing_extensions import Self\n",
    "\n",
    "from pydantic import BaseModel, model_validator\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Trip(BaseModel):\n",
    "    id: int\n",
    "    tpep_pickup_datetime: datetime = datetime.now()\n",
    "    tpep_dropoff_datetime: datetime = datetime.now()\n",
    "    trip_distance: float = -1.0\n",
    "    fare_amount: float = -1.0\n",
    "    pickup_zip: int = -1\n",
    "    dropoff_zip: int = -1\n",
    "\n",
    "    @model_validator(mode='after')\n",
    "    def enforce_different_zips(self) -> Self:\n",
    "        if self.pickup_zip == self.dropoff_zip:\n",
    "            raise ValueError('pickup_zip and dropoff_zip must be different')\n",
    "        return self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0c57177-a0a7-4c2c-bd15-33ff59f0ff6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The test should fail now. That's expected, though! üòú\n",
    "\n",
    "Let's fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8301608a-d341-4d84-ad9f-5d537e4c9826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test Invalid Trips\n",
    "\n",
    "Extend the test to assert that only valid trips are accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "982cfd3b-b151-488c-b188-e9edd65e84f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```py\n",
    "from pydantic_workflow.trip import Trip\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "def test_valid_trip():\n",
    "    Trip(id=10, pickup_zip=10103, dropoff_zip=10110)\n",
    "\n",
    "\n",
    "def test_invalid_trip():\n",
    "    with pytest.raises(ValueError):\n",
    "        Trip(id=10, pickup_zip=10023, dropoff_zip=10023)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f3b446-4114-44fe-8b17-582bf5d2d986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "‚ùØ uv run pytest\n",
    "================================================= test session starts =================================================\n",
    "platform darwin -- Python 3.12.3, pytest-8.3.4, pluggy-1.5.0\n",
    "rootdir: /Users/jacek/dev/learn-databricks/demo/pydantic_workflow\n",
    "configfile: pyproject.toml\n",
    "collected 2 items\n",
    "\n",
    "tests/test_trip.py ..                                                                                           [100%]\n",
    "\n",
    "================================================== 2 passed in 0.09s ==================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c05ad15a-22b8-4aa1-9f06-d7a6993a1c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "It works! ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be6f49a-99fa-4148-ba1c-c0cde7f3b27b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (Optional) Run Job\n",
    "\n",
    "Re-deploy the bundle and run the job to validate that the changes didn't get into our way.\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "‚ùØ databricks bundle run pydantic_workflow_job\n",
    "Run URL: https://curriculum-dev.cloud.databricks.com/?o=3551974319838082#job/706677441584698/run/274782797890396\n",
    "\n",
    "2025-02-16 22:55:55 \"[dev jacek] pydantic_workflow_job\" RUNNING\n",
    "2025-02-16 22:56:20 \"[dev jacek] pydantic_workflow_job\" TERMINATED SUCCESS\n",
    "Output:\n",
    "=======\n",
    "Task notebook_task:\n",
    "\n",
    "=======\n",
    "Task main_task:\n",
    "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
    "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|pickup_zip|dropoff_zip|\n",
    "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
    "| 2016-02-13 21:47:53|  2016-02-13 21:57:15|          1.4|        8.0|     10103|      10110|\n",
    "| 2016-02-13 18:29:09|  2016-02-13 18:37:23|         1.31|        7.5|     10023|      10023|\n",
    "| 2016-02-06 19:40:58|  2016-02-06 19:52:32|          1.8|        9.5|     10001|      10018|\n",
    "| 2016-02-12 19:06:43|  2016-02-12 19:20:54|          2.3|       11.5|     10044|      10111|\n",
    "| 2016-02-23 10:27:56|  2016-02-23 10:58:33|          2.6|       18.5|     10199|      10022|\n",
    "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023b84aa-ce09-4845-aeff-da863fb4c9c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Create PySpark UDF\n",
    "\n",
    "That's the gist of this meetup.\n",
    "\n",
    "From [Scalar UDFs](https://docs.databricks.com/en/udf/index.html#scalar-udfs):\n",
    "\n",
    "> Scalar UDFs operate on a single row and return a single value for each row.\n",
    "\n",
    "Learn more in [What are user-defined functions (UDFs)?](https://docs.databricks.com/en/udf/index.html) (specifically [Scalar UDFs](https://docs.databricks.com/en/udf/index.html#scalar-udfs))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91668d8f-336b-4373-a0a6-6d1ad987a702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# üí° Ideas for Future Events\n",
    "\n",
    "1. [Delta Live Tables](https://docs.databricks.com/en/delta-live-tables/index.html) with uv and pydantic\n",
    "1. Explore more [Pydantic](https://docs.pydantic.dev/latest/) features\n",
    "1. Create a new DAB template with `uv` as the project management tool (based on `default-python` template). Start from `databricks bundle init --help`.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Meetup_next",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
