{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d1e43d-52ad-4ab1-9ae1-a0840c154d0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Databricks Asset Bundles\n",
    "\n",
    "**tl;dr** Automating deployments of Databricks assets (e.g., experiments, jobs, models, and pipelines)\n",
    "\n",
    "Learn more in the official documentation:\n",
    "\n",
    "1. [What are Databricks Asset Bundles?](https://docs.databricks.com/en/dev-tools/bundles/index.html)\n",
    "1. [Develop on Databricks](https://docs.databricks.com/en/languages/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65945cb6-7324-4999-8941-a01281b28e51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "[Databricks Asset Bundles](https://www.databricks.com/resources/demos/tours/data-engineering/databricks-asset-bundles):\n",
    "\n",
    "> Databricks Asset Bundles (DAB) is a new capability on Databricks that **standardizes and unifies the deployment strategy** for all data products developed on the platform.\n",
    "> It allows developers to describe the infrastructure and resources of their project through a **YAML configuration file**.\n",
    "\n",
    "The main take-aways from the above introduction about DAB are as follows:\n",
    "\n",
    "1. DAB is all about standardizing deployment of Databricks projects\n",
    "1. DAB is an [Infrastructure as code (IaC)](https://en.wikipedia.org/wiki/Infrastructure_as_code) tool\n",
    "1. DAB uses a YAML configuration file to declaratively describe what/when/how\n",
    "\n",
    "[Databricks Asset Bundles went GA](https://www.databricks.com/blog/announcing-general-availability-databricks-asset-bundles) around April 23, 2024 ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e39a28-4468-4ee2-8bab-9d0ba3e146f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "With DABs, you can easily bundle deployable resources (jobs, pipelines, notebooks, code) so you can version, test, deploy, and collaborate on your project as a unit.\n",
    "\n",
    "DABs help you adopt software engineering best practices for your projects on the Databricks Platform. DABs facilitate source control, code review, testing, and continuous integration and delivery (CI/CD) for all your data assets as code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c61ceef-34af-4277-8c04-fad1992b9023",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "The [slides](https://docs.google.com/presentation/d/1bnnTR19j_nZhB0bDCMoGga-8Sq6eBjhBAom-6NJ6F0I/edit) of the talk on Databricks Asset Bundles at Data & AI Summit 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d4433a2-b627-4e43-8364-01506756bf6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "[Databricks Asset Bundle deployment modes](https://docs.databricks.com/en/dev-tools/bundles/deployment-modes.html):\n",
    "\n",
    "> Bundles enable programmatic management of Databricks Workflows\n",
    "\n",
    "Databricks asset bundles make it possible to express complete data, analytics, and ML projects as a collection of source files called a bundle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f314bb1-c042-4421-93ab-ddbc7a54335f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Automate Databricks Deployments\n",
    "\n",
    "DAB is not alone in the IaC/deployment 'market' for Databricks.\n",
    "\n",
    "Developers (and devops) have been using the following for quite some time:\n",
    "\n",
    "1. [Databricks REST API](https://docs.databricks.com/api/)\n",
    "1. [Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/index.html)\n",
    "1. [Databricks Terraform provider](https://docs.databricks.com/en/dev-tools/terraform/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a271d5fc-9542-49c2-8687-121140ea66a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fun Fact: DAB == terraform\n",
    "\n",
    "Note `terraform apply` in the output of `databricks bundle deploy`. \n",
    "\n",
    "```\n",
    "Starting resource deployment\n",
    "Error: terraform apply: exit status 1\n",
    "\n",
    "Error: cannot create job: Invalid quartz_cron_expression: '44 37 8 * * ?'. Databricks uses Quartz cron syntax, which is different from the standard cron syntax. See https://docs.databricks.com/jobs.html#schedule-a-job  for more details.\n",
    "\n",
    "  with databricks_job.jacek_demo_meetup_job,\n",
    "  on bundle.tf.json line 82, in resource.databricks_job.jacek_demo_meetup_job:\n",
    "  82:       }\n",
    "```\n",
    "\n",
    "```console\n",
    "$ databricks bundle validate\n",
    "{\n",
    "  \"bundle\": {\n",
    "    \"name\": \"delta_live_tables_demo\",\n",
    "    \"target\": \"dev\",\n",
    "    \"environment\": \"dev\",\n",
    "    \"terraform\": { ‚¨ÖÔ∏è\n",
    "      \"exec_path\": \"/Users/jacek/dev/oss/learn-databricks/Databricks Asset Bundles/delta_live_tables_demo/.databricks/bundle/dev/bin/terraform\"\n",
    "    },\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41a21be4-a9d8-4087-9a66-8b0849e6c843",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "[workspace](https://docs.databricks.com/en/dev-tools/bundles/settings.html#workspace) (highlighting mine):\n",
    "\n",
    "> The `state_path` mapping defaults to the default path of `${workspace.root}/state` and represents the path within your workspace to store **Terraform** state information about deployments.\n",
    "\n",
    "> **Note:**\n",
    ">\n",
    "> _\"to store Terraform state information about deployments\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f80e43de-8a63-4fcb-b3a5-bdbaf93c06e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Typical Development Flow\n",
    "\n",
    "Typical development flow using Databricks Asset Bundles (`databricks bundle` commands):\n",
    "\n",
    "* `init`\n",
    "* `deploy`\n",
    "* `run`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c90739-b6dc-4a1e-b379-67741e383c65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## üöÄ Demo: On Fast Track to Deploy\n",
    "\n",
    "[Develop a job on Databricks by using Databricks asset bundles](https://docs.databricks.com/en/workflows/jobs/how-to/use-bundles-with-jobs.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccba4a5e-9cad-4c60-b576-96bec228b754",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```console\n",
    "$ databricks --version\n",
    "Databricks CLI v0.221.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e531d9-44cb-4793-910d-1c23fc10b5df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```shell\n",
    "$ databricks bundle\n",
    "Databricks Asset Bundles let you express data/AI/analytics projects as code.\n",
    "\n",
    "Online documentation: https://docs.databricks.com/en/dev-tools/bundles/index.html\n",
    "\n",
    "Usage:\n",
    "  databricks bundle [command]\n",
    "\n",
    "Available Commands:\n",
    "  deploy      Deploy bundle\n",
    "  deployment  Deployment related commands\n",
    "  destroy     Destroy deployed bundle resources\n",
    "  generate    Generate bundle configuration\n",
    "  init        Initialize using a bundle template\n",
    "  run         Run a job or pipeline update\n",
    "  schema      Generate JSON Schema for bundle configuration\n",
    "  sync        Synchronize bundle tree to the workspace\n",
    "  validate    Validate configuration\n",
    "\n",
    "Flags:\n",
    "  -h, --help          help for bundle\n",
    "      --var strings   set values for variables defined in bundle config. Example: --var=\"foo=bar\"\n",
    "\n",
    "Global Flags:\n",
    "      --debug            enable debug logging\n",
    "  -o, --output type      output type: text or json (default text)\n",
    "  -p, --profile string   ~/.databrickscfg profile\n",
    "  -t, --target string    bundle target to use (if applicable)\n",
    "\n",
    "Use \"databricks bundle [command] --help\" for more information about a command.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a6eb30-987e-47a2-908d-f79d41abe485",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### init\n",
    "\n",
    "<br>\n",
    "\n",
    "```shell\n",
    "$ databricks bundle init\n",
    "Search: ‚ñà\n",
    "? Template to use:\n",
    "  default-python (The default Python template for Notebooks / Delta Live Tables / Workflows)\n",
    "  default-sql\n",
    "  dbt-sql\n",
    "  mlops-stacks\n",
    "  custom...\n",
    "```\n",
    "\n",
    "> ‚ö†Ô∏è **Note:**\n",
    ">\n",
    "> Two new templates added: `default-sql` and `dbt-sql`.\n",
    "\n",
    "Select `default-python`.\n",
    "\n",
    "```shell\n",
    "Welcome to the default Python template for Databricks Asset Bundles!\n",
    "Please provide the following details to tailor the template to your preferences.\n",
    "\n",
    "Unique name for this project [my_project]:\n",
    "```\n",
    "\n",
    "...and accept the defaults (except to the stub (sample) Python package).\n",
    "\n",
    "```shell\n",
    "Include a stub (sample) notebook in 'my_project/src': yes\n",
    "Include a stub (sample) Delta Live Tables pipeline in 'my_project/src': yes\n",
    "Include a stub (sample) Python package in 'my_project/src': no\n",
    "Workspace to use (auto-detected, edit in 'job_id_change/databricks.yml'): https://XXX\n",
    "\n",
    "‚ú® Your new project has been created in the 'my_project' directory!\n",
    "\n",
    "Please refer to the README.md file for \"getting started\" instructions.\n",
    "See also the documentation at https://docs.databricks.com/dev-tools/bundles/index.html.\n",
    "```\n",
    "\n",
    "> ‚ö†Ô∏è **Note:**\n",
    ">\n",
    "> Project name must consist of letters, numbers, and underscores\n",
    "\n",
    "> ‚ö†Ô∏è **Note:**\n",
    ">\n",
    "> Workspace to use (auto-detected, edit in 'job_id_change/databricks.yml')\n",
    "\n",
    "```shell\n",
    "$ cd my_project\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b18be01-b495-42c5-bf75-5779b0f0f31e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### deploy\n",
    "\n",
    "<br>\n",
    "\n",
    "```shell\n",
    "$ databricks bundle deploy\n",
    "Uploading bundle files to /Users/jacek@japila.pl/.bundle/my_project/dev/files...\n",
    "Deploying resources...\n",
    "Updating deployment state...\n",
    "Deployment complete!\n",
    "```\n",
    "\n",
    "> ‚ö†Ô∏è **Note:**\n",
    ">\n",
    "> Uploading bundle files to /Users/jacek@japila.pl/.bundle/my_project/dev/files..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "030c2ee3-a20d-4378-929d-bbe5921079ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### run\n",
    "\n",
    "<br>\n",
    "\n",
    "```console\n",
    "$ databricks bundle run --help\n",
    "Run the job or pipeline identified by KEY.\n",
    "\n",
    "The KEY is the unique identifier of the resource to run. In addition to\n",
    "customizing the run using any of the available flags, you can also specify\n",
    "keyword or positional arguments as shown in these examples:\n",
    "\n",
    "   databricks bundle run my_job -- --key1 value1 --key2 value2\n",
    "\n",
    "Or:\n",
    "\n",
    "   databricks bundle run my_job -- value1 value2 value3\n",
    "\n",
    "If the specified job uses job parameters or the job has a notebook task with\n",
    "parameters, the first example applies and flag names are mapped to the\n",
    "parameter names.\n",
    "\n",
    "If the specified job does not use job parameters and the job has a Python file\n",
    "task or a Python wheel task, the second example applies.\n",
    "\n",
    "Usage:\n",
    "  databricks bundle run [flags] KEY\n",
    "\n",
    "Job Flags:\n",
    "      --params stringToString   comma separated k=v pairs for job parameters (default [])\n",
    "\n",
    "Job Task Flags:\n",
    "  Note: please prefer use of job-level parameters (--param) over task-level parameters.\n",
    "  For more information, see https://docs.databricks.com/en/workflows/jobs/create-run-jobs.html#pass-parameters-to-a-databricks-job-task\n",
    "      --dbt-commands strings                 A list of commands to execute for jobs with DBT tasks.\n",
    "      --jar-params strings                   A list of parameters for jobs with Spark JAR tasks.\n",
    "      --notebook-params stringToString       A map from keys to values for jobs with notebook tasks. (default [])\n",
    "      --pipeline-params stringToString       A map from keys to values for jobs with pipeline tasks. (default [])\n",
    "      --python-named-params stringToString   A map from keys to values for jobs with Python wheel tasks. (default [])\n",
    "      --python-params strings                A list of parameters for jobs with Python tasks.\n",
    "      --spark-submit-params strings          A list of parameters for jobs with Spark submit tasks.\n",
    "      --sql-params stringToString            A map from keys to values for jobs with SQL tasks. (default [])\n",
    "\n",
    "Pipeline Flags:\n",
    "      --full-refresh strings   List of tables to reset and recompute.\n",
    "      --full-refresh-all       Perform a full graph reset and recompute.\n",
    "      --refresh strings        List of tables to update.\n",
    "      --refresh-all            Perform a full graph update.\n",
    "      --validate-only          Perform an update to validate graph correctness.\n",
    "\n",
    "Flags:\n",
    "  -h, --help      help for run\n",
    "      --no-wait   Don't wait for the run to complete.\n",
    "      --restart   Restart the run if it is already running.\n",
    "\n",
    "Global Flags:\n",
    "      --debug            enable debug logging\n",
    "  -o, --output type      output type: text or json (default text)\n",
    "  -p, --profile string   ~/.databrickscfg profile\n",
    "  -t, --target string    bundle target to use (if applicable)\n",
    "      --var strings      set values for variables defined in bundle config. Example: --var=\"foo=bar\"\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "```shell\n",
    "$ databricks bundle run\n",
    "Update URL: https://training-partners.cloud.databricks.com/#joblist/pipelines/84f3895d-a910-4d9a-b8ec-ac275d4985bd/updates/ce459b1d-5323-46de-b0a4-86b459c13301\n",
    "\n",
    "2023-10-21T12:58:16.972Z update_progress INFO \"Update ce459b is WAITING_FOR_RESOURCES.\"\n",
    "2023-10-21T13:01:50.065Z update_progress INFO \"Update ce459b is INITIALIZING.\"\n",
    "2023-10-21T13:02:36.634Z update_progress INFO \"Update ce459b is SETTING_UP_TABLES.\"\n",
    "2023-10-21T13:03:01.865Z update_progress INFO \"Update ce459b is RUNNING.\"\n",
    "2023-10-21T13:03:01.871Z flow_progress   INFO \"Flow 'filtered_taxis' is QUEUED.\"\n",
    "2023-10-21T13:03:01.893Z flow_progress   INFO \"Flow 'filtered_taxis' is PLANNING.\"\n",
    "2023-10-21T13:03:02.673Z flow_progress   INFO \"Flow 'filtered_taxis' is STARTING.\"\n",
    "2023-10-21T13:03:02.712Z flow_progress   INFO \"Flow 'filtered_taxis' is RUNNING.\"\n",
    "2023-10-21T13:03:42.162Z flow_progress   INFO \"Flow 'filtered_taxis' has COMPLETED.\"\n",
    "2023-10-21T13:03:43.702Z update_progress INFO \"Update ce459b is COMPLETED.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fc56a7-0e48-45a3-a7e0-5cbac122f56f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Validate configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2653416e-49bb-4e63-81a8-bffa6eddbb35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "From [Databricks Asset Bundle configurations](https://docs.databricks.com/en/dev-tools/bundles/settings.html):\n",
    "\n",
    "1. A bundle configuration file must be expressed in YAML format\n",
    "1. A bundle configuration file must contain at minimum the top-level [bundle](https://docs.databricks.com/en/dev-tools/bundles/settings.html#bundle-syntax-mappings-bundle) mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c4eef0-07e4-4957-ae5c-362d54ff160a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```console\n",
    "$ databricks bundle validate\n",
    "Error: unable to locate bundle root: databricks.yml not found\n",
    "```\n",
    "\n",
    "```console\n",
    "$ databricks bundle validate --help\n",
    "Validate configuration\n",
    "\n",
    "Usage:\n",
    "  databricks bundle validate [flags]\n",
    "\n",
    "Flags:\n",
    "  -h, --help   help for validate\n",
    "\n",
    "Global Flags:\n",
    "      --log-file file            file to write logs to (default stderr)\n",
    "      --log-format type          log output format (text or json) (default text)\n",
    "      --log-level format         log level (default disabled)\n",
    "  -o, --output type              output type: text or json (default text)\n",
    "  -p, --profile string           ~/.databrickscfg profile\n",
    "      --progress-format format   format for progress logs (append, inplace, json) (default default)\n",
    "  -t, --target string            bundle target to use (if applicable)\n",
    "      --var strings              set values for variables defined in bundle config. Example: --var=\"foo=bar\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1baca7c-4ece-493c-8351-96c823ab3934",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Variables\n",
    "\n",
    "[Custom variables](https://docs.databricks.com/en/dev-tools/bundles/settings.html#custom-variables):\n",
    "\n",
    "* Use custom variables to make your bundle configuration files more modular and reusable\n",
    "* Variables work only with string-based values.\n",
    "* E.g., the ID of an existing cluster for various workflow runs within multiple targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572f396d-a36a-46ab-9880-9bc432034b47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "`variables` mapping in a bundle configuration file\n",
    "\n",
    "```yaml\n",
    "variables:\n",
    "  <variable-name>:\n",
    "    description: <optional-description>\n",
    "    default: <optional-default-value>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb69c558-d520-465d-af19-3600109db3b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "* You should provide the same values during both the deployment and run stages\n",
    "* For variables, use substitutions in the format `${var.<variable_name>}`\n",
    "* Use Databricks CLI's `--var` option to define the value of a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1bd90e-e285-4357-b974-71a847386814",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```shell\n",
    "databricks bundle deploy --var \"quartz_cron_expression=1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3249d2f4-a159-4624-aa93-48837b1bdf0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ‚òÄÔ∏è Demo: Delta Live Tables Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872f878b-c1ae-40a6-990f-8c3bbb5e5ed5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```console\n",
    "$ databricks bundle init\n",
    "Template to use [default-python]:\n",
    "Unique name for this project [my_project]: delta_live_tables_demo\n",
    "Include a stub (sample) notebook in 'delta_live_tables_demo/src': yes\n",
    "Include a stub (sample) Delta Live Tables pipeline in 'delta_live_tables_demo/src': yes\n",
    "Include a stub (sample) Python package in 'delta_live_tables_demo/src': yes\n",
    "\n",
    "‚ú® Your new project has been created in the 'delta_live_tables_demo' directory!\n",
    "\n",
    "Please refer to the README.md of your project for further instructions on getting started.\n",
    "Or read the documentation on Databricks Asset Bundles at https://docs.databricks.com/dev-tools/bundles/index.html.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27db2640-827a-4ffa-9f69-b1a42e36b2fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```console\n",
    "$ databricks auth profiles --help\n",
    "Lists profiles from ~/.databrickscfg\n",
    "\n",
    "Usage:\n",
    "  databricks auth profiles [flags]\n",
    "\n",
    "Flags:\n",
    "  -h, --help            help for profiles\n",
    "      --skip-validate   Whether to skip validating the profiles\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c79682e-c670-41ea-9756-7343cef57fe4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```console\n",
    "$ databricks auth profiles\n",
    "Name     Host                                            Valid\n",
    "DEFAULT  https://training-partners.cloud.databricks.com  YES\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e111e641-5d4f-46ea-bbe8-eb345162f132",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```console\n",
    "// Uses default target\n",
    "// default: true\n",
    "$ databricks bundle validate\n",
    "{\n",
    "  \"bundle\": {\n",
    "    \"name\": \"delta_live_tables_demo\",\n",
    "    \"target\": \"dev\",\n",
    "    \"environment\": \"dev\",\n",
    "    \"terraform\": {\n",
    "      \"exec_path\": \"/Users/jacek/dev/oss/learn-databricks/Databricks Asset Bundles/delta_live_tables_demo/.databricks/bundle/dev/bin/terraform\"\n",
    "    },\n",
    "    \"lock\": {\n",
    "      \"enabled\": null,\n",
    "      \"force\": false\n",
    "    },\n",
    "    \"force\": false,\n",
    "    \"git\": {\n",
    "      \"branch\": \"meetup-nov-2\",\n",
    "      \"origin_url\": \"https://github.com/jaceklaskowski/learn-databricks.git\",\n",
    "      \"commit\": \"63f784b0000e85107ffea06be24c8151d45cc6c7\"\n",
    "    },\n",
    "    \"mode\": \"development\"\n",
    "  },\n",
    "  ...\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683bdb9d-680c-4fb8-b10c-6a06b79ee8a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```console\n",
    "$ databricks bundle validate --target prod\n",
    "{\n",
    "  \"bundle\": {\n",
    "    \"name\": \"delta_live_tables_demo\",\n",
    "    \"target\": \"prod\",\n",
    "    \"environment\": \"prod\", ‚¨ÖÔ∏è\n",
    "    \"terraform\": {\n",
    "      \"exec_path\": \"/Users/jacek/dev/oss/learn-databricks/Databricks Asset Bundles/delta_live_tables_demo/.databricks/bundle/prod/bin/terraform\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae130386-39a3-4d43-ae46-e4f84e0e6912",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Review `resources/delta_live_tables_demo_pipeline.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa89c6d-cd99-4c9f-8ad8-446d02a3462e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Deployment Modes\n",
    "\n",
    "[Databricks Asset Bundle deployment modes](https://docs.databricks.com/en/dev-tools/bundles/deployment-modes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dfc9af6-bc7c-4947-a80a-a1a704997000",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "1. In CI/CD workflows, developers typically code, test, deploy, and run solutions in various phases, or modes.\n",
    "1. The most common deployment modes include:\n",
    "    * A development mode for pre-production validation\n",
    "    * A production mode for validated deliverables\n",
    "1. Databricks Asset Bundles provides an optional collection of default behaviors that correspond to each of these modes.1. Modes specify (declaratively) intended behaviors\n",
    "1. `mode` mapping in a target (under `targets`)\n",
    "    * `databricks bundle deploy -t <target-name>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09226a6-d5d4-47b5-82d0-56c2678ac10a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Development mode\n",
    "\n",
    "[Development mode](https://docs.databricks.com/en/dev-tools/bundles/deployment-modes.html#development-mode)\n",
    "\n",
    "1. `mode: development`\n",
    "1. Tags deployed jobs and pipelines with a `dev` Databricks tag\n",
    "1. Delta Live Tables pipelines run in `development: true`\n",
    "1. _others_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5800a736-d63b-45f5-8efc-e2ece0d96d12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Production mode\n",
    "\n",
    "[Production mode](https://docs.databricks.com/en/dev-tools/bundles/deployment-modes.html#production-mode)\n",
    "\n",
    "1. `mode: production`\n",
    "1. Validates that all related deployed Delta Live Tables pipelines are marked as `development: false`.\n",
    "1. Validates that the current git branch is equal to the git branch that is specified in the target\n",
    "      ```\n",
    "      git:\n",
    "        branch: main\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643be651-4bd1-4483-b2a6-d6d9c20d9b8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bundle Templates\n",
    "\n",
    "[Databricks Asset Bundle templates](https://docs.databricks.com/en/dev-tools/bundles/templates.html)\n",
    "\n",
    "`databricks bundle init` accepts an optional path of the template to use to initialize a DAB project:\n",
    "- `default-python` for the default Python template\n",
    "- a local file system path with a template directory\n",
    "- a git repository URL, e.g. https://github.com/my/repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbc3ca0-8177-453d-b6e3-810fbf4e9ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "```shell\n",
    "$ databricks bundle init --help\n",
    "Initialize using a bundle template.\n",
    "\n",
    "TEMPLATE_PATH optionally specifies which template to use. It can be one of the following:\n",
    "- 'default-python' for the default Python template\n",
    "- a local file system path with a template directory\n",
    "- a Git repository URL, e.g. https://github.com/my/repository\n",
    "\n",
    "See https://docs.databricks.com//dev-tools/bundles/templates.html for more information on templates.\n",
    "\n",
    "Usage:\n",
    "  databricks bundle init [TEMPLATE_PATH] [flags]\n",
    "\n",
    "Flags:\n",
    "      --config-file string    File containing input parameters for template initialization.\n",
    "  -h, --help                  help for init\n",
    "      --output-dir string     Directory to write the initialized template to.\n",
    "      --template-dir string   Directory path within a Git repository containing the template.\n",
    "\n",
    "Global Flags:\n",
    "      --log-file file            file to write logs to (default stderr)\n",
    "      --log-format type          log output format (text or json) (default text)\n",
    "      --log-level format         log level (default disabled)\n",
    "  -o, --output type              output type: text or json (default text)\n",
    "  -p, --profile string           ~/.databrickscfg profile\n",
    "      --progress-format format   format for progress logs (append, inplace, json) (default default)\n",
    "  -t, --target string            bundle target to use (if applicable)\n",
    "      --var strings              set values for variables defined in bundle config. Example: --var=\"foo=bar\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba77dbee-b15e-4a23-9c5a-ac7b6a024076",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## üöÄ Demo: Create DAB Template (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988e6d48-e2a1-41d7-a09c-b25f9db069a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "An idea is to execute the following command with a random template name and guide the audience through errors.\n",
    "\n",
    "```\n",
    "databricks bundle init\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ffd423-38f3-4207-b0f6-5a4008a83667",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Source Code\n",
    "\n",
    "Given [this recent PR](https://github.com/databricks/cli/pull/795/files), it appears that the source code of `bundle` command of Databricks CLI is in [Databricks CLI](https://github.com/databricks/cli/tree/main/cmd/bundle) repo itself.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> Phew, the source code is Go! üò¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f55945f2-2e7c-42b2-ac90-e1e8fe43a055",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Bundle Configuration File (databricks.yml)\n",
    "\n",
    "‚û°Ô∏è [Databricks Asset Bundle configurations](https://docs.databricks.com/en/dev-tools/bundles/settings.html#overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d06f4325-1021-4354-9007-edc6a9f7e68b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Target mappings\n",
    "\n",
    "[target mappings](https://docs.databricks.com/en/dev-tools/bundles/settings.html#targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b504bf1-ea06-4527-8fe4-58c5e201e7d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Target Workspace Resolution\n",
    "\n",
    "<br>\n",
    "\n",
    "Databricks CLI uses `-t, --target string` options for the target Databricks workspace\n",
    "\n",
    "[Databricks Asset Bundle configurations](https://docs.databricks.com/en/dev-tools/bundles/settings.html#examples):\n",
    "\n",
    "* Databricks recommends that you use the `host` mapping instead of the default mapping wherever possible (makes your bundle configuration files more portable)\n",
    "* Setting the `host` mapping instructs the Databricks CLI to find a matching profile in your `~/.databrickscfg` file and then use that profile‚Äôs fields to determine which Databricks authentication type to use\n",
    "* If multiple profiles with a matching host field exist within your `~/.databrickscfg` file, then you must use the profile to instruct the Databricks CLI about which specific profile to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff35b9d5-293f-43f4-a4a0-68a11880f8b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Resource ID Resolution\n",
    "\n",
    "[Retrieve an object‚Äôs ID value](https://docs.databricks.com/en/dev-tools/bundles/settings.html#retrieve-an-objects-id-value):\n",
    "\n",
    "* For the `alert`, `cluster_policy`, `cluster`, `dashboard`, `instance_pool`, `job`, `metastore`, `pipeline`, `query`, `service_principal`, and `warehouse` object types, you can define a `lookup` for your custom variable to retrieve a named object‚Äôs ID\n",
    "* The correct resolved ID of an object is always used for the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35164ce9-e106-49c6-b4ed-4f2881a96286",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Project Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df9929b9-2527-46f9-96e2-4abacb80068d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Fixtures\n",
    "\n",
    "This folder is reserved for (test) fixtures.\n",
    "\n",
    "Learn more in [Unit Testing and Code Modularization in Databricks](https://medium.com/@mariusz_kujawski/unit-testing-and-code-modularization-in-databricks-33f40c9f6da9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1afd2bf4-6a29-4db4-ae44-2bfc6ee0d759",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e982e851-d1d7-4a5d-b7da-c6f38422c47d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "1. Any relationship between DAB and Databricks SDK?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Asset Bundles",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
